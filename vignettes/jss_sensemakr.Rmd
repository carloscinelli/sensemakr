---
documentclass: jss
author:
  - name: Carlos Cinelli 
    affiliation: University of California, Los Angeles
    address: >
      Department of Statistics,
      8125 Math Sciences Building,
      Los Angeles, CA 90095, USA.
    email: \email{carloscinelli@ucla.edu}
    url: http://carloscinelli.com
  - name: Jeremy Ferwerda
    affiliation: Dartmouth College
    address: >
      Department of Government, 
      Hanover, NH 03755
    email: \email{jeremy.a.ferwerda@dartmouth.edu}
    url: http://jeremyferwerda.com/
  - name: Chad Hazlett
    affiliation: University of California, Los Angeles
    address: >
      Department of Statistics, 
      8125 Math Sciences Building, 
      Los Angeles, CA 90095, USA.
    email: \email{chazlett@ucla.edu}
    url: http://chadhazlett.com
title:
  # If you use tex in the formatted title, also supply version without
  # For running headers, if needed
  formatted: "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
  plain:     "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
  short:     "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
abstract: "This paper introduces the \\proglang{R} and \\proglang{Stata} package \\pkg{sensemakr} for assesing the sensitivity of regression estimates to unobserved confounding. The package provides a suite of tools for sensitivity analysis in regression models developed in @cinelli:jrssb2019. These tools are based on the familiar omitted variable bias framework, and can be easily computed using only standard regression results. Furthermore, they do not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handle multiple confounders, possibly acting non-linearly, and enable bounding of sensitivity parameters employing domain knowledge."
keywords:
  # at least one keyword must be supplied
  formatted: [causal inference, sensitivity analysis, omitted variable bias, robustness value, R, Stata]
  plain:     [causal inference, sensitivity analysis, omitted variable bias, robustness value, R, Stata]
preamble: >
  \usepackage{amsmath}
output: 
  rticles::jss_article:
    fig_caption: yes
    keep_tex: true
bibliography: sensemakr.bib
biblio-style: jss      #Added Citation style is listed to use in JSS Instructions for Authors.
editor_options: 
  chunk_output_type: console
graphics: yes
---

```{r setup, include=FALSE}
library(knitr)
library(stargazer)

rm(list = ls())

# Set default plot sizes
## - fig.width and fig.heights are R's plot sizes,
## The default options of contours 
## works best with heigh and width around 4 to 5
## - out.width and out.height just resize the image
## 250px seems optimal, and takes less than half a page.
## for extreme plots, change manually fig.width to 6 and out.width to 350px
opts_chunk$set(fig.width  = 4.5, 
               fig.height = 4.5, 
               fig.pos    = "!tp",
               message    = FALSE, 
               out.width  = '250px', 
               out.height = '250px',
               fig.align  = "center")

opts_chunk$set(tidy = FALSE)
options(scipen = 999, digits = 3)
hook_output = knit_hooks$get('output')

knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  n <- options$linewidth
  if (!is.null(n)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```



# Introduction {#intro}

<!-- Investigating the causal effect of some variable (the "treatment") on another (the "outcome") using observational data poses a perenial problem for researchers across disciplines.  -->

Investigators in a wide range of disciplines face the perennial challenge of making and defending causal claims using observational sources of data in which they could not ensure the "treatment" variable of interest was randomized or otherwise free of confounding. The most common identification strategy, in such circumstances, is to condition on or "adjust for" observed covariates in the hopes that there are not problematic unobserved covariates that remain. Linear regression remains among the most widely used approaches to realize such adjustments.  However, supporting the claim that the coefficient from such a regression unbiasedly estimates a causal quantity requires defending an untestable assumption that there are no unobserved confounders. Moreover, such an assumption does not need to hold precisely (and never does) in order for a research result to remain substantively valid and informative. Sensitivity analyses quantify how strong unobserved confounding needs to be to change the research conclusions. They also aid in reasoning as whether confounding of such strength is plausible. This gives investigators the ability to rigorously describe the fragility of putative causal estimates in the face of potential unobserved confounding.

<!--, and \emph{qualitative} debates of whether or not \emph{any} unobserved confounding exists are generally not productive---it always does. Therefore, a more useful discussion is \emph{quantitative}: how strong would unobserved confounding need to be to change the research conclusions? Are confounders with such strength plausible? Sensitivity analysis allows us to quantitatively discuss the fragility of putative causal estimates when the underlying assumption of no unobserved confounding is challenged.
!-->

The goal of this paper is to introduce the \proglang{R} and \proglang{Stata} package \pkg{sensemakr} \citep{sensemakr:R,sensemakr:stata}, which implements a suite of sensitivity analysis tools proposed in @cinelli:jrssb2019 for assessing the sensitivity of a regression coefficient to the inclusion of omitted variables. The goal of \pkg{sensemakr} is to make it easy to understand the impact that omitted variables would have on a regression result. This allows analysts to investigate the robustness of their estimates to violations of the assumption of no unobserved confounding, answering questions such as:

- How strong would an unobserved confounder (or a group of confounders) have to be to change our research conclusions?

- In a worst-case scenario, how robust are our results to all unobserved confounders acting together, possibly non-linearly?

- How strong would confounding need to be relative to the strength of observed covariates, to change our answer by a certain amount?



<!-- Weaker claims, e.g. that the causal effect has a particular sign, similarly require the untestable assumption that unobserved confounding does not exceed a certain level. Sensitivity analyses aid in evaluating such claims, by equiping investigators with information about how sensitive a given result is to varying degrees of unobserved confounding, thereby allowing a quantitative discussion regarding the fragility of putative causal estimates when the underlying assumption of no unobserved confounding is challenged. -->

<!-- improving upon our ability to understand and transparently communicate results under confounding. -->


<!-- In the common setting where researchers "adjust for observables" using tools such as regression,  claiming that the resulting estimate is unbiased for a causal quantity rely on untestable assumptions about the absence of unobserved confounders. Weaker claims, e.g. that the causal quantity has a particular sign, similarly require that unobserved confounding does not exceed a certain level. Sensitivity analyses aid in evaluating such claims, by equiping investigators with information about how sensitive a given result is to varying degrees of unobserved confounding, improving upon our ability to understand and transparently communicate results under confounding.  -->


Various sensitivity analyses have been proposed dating back to @cornfield1959smoking, with more recent contributions including [@rosenbaum1983assessing; @robins1999association; @frank:smr2000; @rosenbaum2002gamma; @imbens2003sensitivity; @brumback2004sensitivity; @frank:eepa2008; @hosman2010sensitivity; @imai2010identification; @arah2011; @blackwell2013selection; @frank2013would; @carnegie:jree2016; @dorie2016flexible; @middleton2016bias; @oster:jbes2017; @cinelli:icml2019; @franks:jasa2019]. Yet, such sensitivity analyses remain underutilized. We argue that a number of factors contribute to this reluctant uptake. One is the complicated nature and strong assumptions many of these methods impose, sometimes involving restrictions on or even a complete description of the nature of the confounder. A second reason is that though users routinely report "regression tables" (or perhaps coefficient plots) to convey the results of a regression, until recently we have lacked "standard" quantities that can simply and correctly summarize sensitivity in the face of unobserved confounding. Third, and most fundamentally, connecting the results of a formal sensitivity analysis to a cogent argument about what types of confounders may exist in one's research project is often difficult, particularly when there are no compelling arguments as to why the treatment assignment should be approximately "ignorable", "exogeneous", or "as-if random".  Further, some of the solutions offered by the literature can lead users to erroneous conclusions.  

This paper is organized as follows. Section \ref{review} briefly reviews the omitted variable bias framework for sensitivity sensitivity analysis proposed in @cinelli:jrssb2019; this framework provides the theoretical foundations for the tools developed in \pkg{sensemakr}. Next, Section \ref{r-basic} goes over the the basic functionality and provides a practical introduction to sensitivity analysis using \proglang{sensemakr} for \proglang{R}. Section \ref{r-adv} looks at advanced usage of the \proglang{R} package, and shows how to leverage  individual functions for customized sensitivity analyses. Finally, Section \ref{stata} describes  \proglang{sensemakr} for \proglang{Stata}. We end the paper with final remarks...


# Sensitivity analysis in an omitted variable bias framework {#review}

In this section, we briefly review the omitted variable bias (OVB) framework for sensitivity analysis presented in @cinelli:jrssb2019. This method builds on a scale-free reparameterization of the OVB formula in terms of partial $R^2$ values. Benefits of the parameterization include:

<!-- CJH: I don't care that much but this last sentence above and the list below strike me as odd here; they are a little in the weeds for a reader who hasn't actually gotten through the setup before. I'd suggest moving to after we give the formulas in terms of R2s.. 
!-->

- assessing the sensitivity of multiple confounders acting together, possibly non-linearly;
- assessing the sensitivity to extreme scenarios in which all (or a designated portion) of the residual variation of the oucome is assumed to be explained by unobserved confounding;
- exploiting knowledge of relative strength of variables to bound the bias due to unobserved confounding;
- presenting all sensitivity results concisely, for easy routine reporting.

<!-- Readers familiar with the method may skip to this section, and read directly the implementation in which we show its implementation with \pkg{sensemakr}. -->

<!--  -->

<!-- This approach shows how the familiar “omitted variable bias” (OVB) framework can be extended to address these challenges in the linear regression setting.  -->
<!-- Notably, all these results do not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounder, and can be used to assess the sensitivity to multiple confounders, whether they influence the treatment and outcome linearly or not.  -->


<!-- Readers that want to see these tools applied in practice, may skip this section. -->


## The OVB framework {#ovb}

The starting point of our analysis is a *full* (or "long") linear regression model of an outcome $Y$ on a treatment $D$, controlling for a set of covariates given by \emph{both} ${\bf X}$ and $Z$,
\begin{align}
Y &= \hat{\tau} D + {\bf X} \hat{{\bf \beta}} +  \hat{\gamma}Z + \hat{\epsilon}_{\text{full}}  \label{eq:fulleq}
\end{align}
\noindent where $Y$ is an $(n \times 1)$ vector containing the outcome of interest for each of the $n$ observations and $D$ is an $(n \times 1)$ treatment variable (which may be continuous or binary); ${\bf X}$ is an $(n \times p)$ matrix of \emph{observed} covariates including the constant; and $Z$ is a single $(n \times 1)$ \emph{unobserved} covariate (we discuss how to extend results for a multivariate $Z$ below).  

Equation \ref{eq:fulleq} is the regresion model that the investigator *wished* she had run to obtain a valid causal estimate of the effect of $D$ on $Y$. Nevertheless,  $Z$ is unobserved. Therefore, the feasible regression the investigator is able to estimate is the *restricted* (or "short") model \emph{omitting} $Z$, that is,
\begin{align}
Y  &= \hat{\tau}_{\text{res}} D + {\bf X} \hat{{\bf \beta}}_{\text{res}} + \hat{\epsilon}_{\text{res}}\label{eq:restrictedeq}
\end{align}

Given the discrepancy of what we wish to know and what we actually have, the main question we would like to answer is: how do the observed point estimate and standard error of the restricted regression, $\hat{\tau}_{\text{res}}$ and $\widehat{se}(\hat{\tau}_{\text{res}})$, compare to the desired point estimate and standard error of the full regression, $\hat{\tau}$ and $\widehat{se}(\hat{\tau})$?

### OVB with the partial R2 parameterization

Define as $\widehat{\text{bias}}$ the difference between the full and restricted  estimates, 

\begin{align}
\widehat{\text{bias}}~:=~\hat{\tau}_{\text{\text{res}}}~-~\hat{\tau}
\end{align}

Now let: (i) $R^2_{D\sim Z | {\bf X}}$ denote the share of residual variance of the *treatment* $D$ explained by the omitted variable $Z$, after accounting for the remaining covariates ${\bf X}$; and, (ii)  $R^2_{Y\sim Z|D, {\bf X}}$ denote the share of residual variance of the *outcome* $Y$ explained by the omitted variable $Z$, after accounting for ${\bf X}$ and $D$. @cinelli:jrssb2019 have shown that these quantities are sufficient for determining the bias, adjusted estimate and adjusted standard errors of the full regression of Equation \ref{eq:fulleq}.  

More precisely, the bias can be written as,

\begin{align}
|\widehat{\text{bias}}| &= \widehat{\text{se}}(\hat{\tau}_{\text{res}}) \sqrt{\frac{ R^2_{Y\sim Z|D, {\bf X}}~ R^2_{D\sim Z | {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} (\text{df})} \label{eq:r2bias2}
\end{align}

Where $\text{df}$ stands for the degrees of freedom of the restricted regression actually run. Moreover, the estimated standard error of $\hat{\tau}$ can be recoverd with,

\begin{align}
\widehat{\text{se}}(\hat{\tau})  = \widehat{\text{se}}(\hat{\tau}_{\text{res}}) \sqrt{\frac{1 - R^2_{Y\sim Z|D, {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} \left(\frac{\text{df}}{\text{df}-1}\right)}.  \label{eq:r2se} 
\end{align}

Given hypothetical values of $R^2_{D\sim Z | {\bf X}}$ and  $R^2_{Y\sim Z|D, {\bf X}}$, Equations \ref{eq:r2bias2} and \ref{eq:r2se} allow investigators to examine the sensitivity of point estimates and standard-errors (and consequently also t-values, confidence intervals or p-values) to the inclusion of any omitted variable $Z$ with such strengths. Or, coversely, given a critical threshold deemed to be problematic, one can find the strength of confounders capable of bringing about a bias of that ammount. Another useful property of the OVB formula with the partial $R^2$ parameterization is that the effect of $R^2_{Y\sim Z|D, {\bf X}}$ on the bias is bounded. This allows investigators to contemplate extreme sensitivity scenarios, in which the parameter $R^2_{Y\sim Z|D, {\bf X}}$ is set to 1 (or another conservativie value), and see what happens as $R^2_{D\sim Z | {\bf X}}$ varies.

## Sensitivity statistics for routine reporting {#rv-and-r2yd}

The previous formulas fully determine the bias (or adjusted values) of the estimate and the standard error for any given degree of confounding, and as such can be used in numerous ways to explore the sensitivity of a regression result. For instance, sensitivity contour plots and sensitivity plots of extreme scenarios, which we demonstrate in the next sections using \pkg{sensemakr}, allow us to fully explore the sensitivity of an estimate as we vary both sensitivity parameters. 

Nevertheless, making sensitivity analysis standard practice benefits from simple and interpretable statistics which can quickly describe the sensitivity of a study to unobserved confounding. These statistics serve two main purposes:

1. They can be easily displayed alongside other usual summary statistics in regression tables, making a minimal sensitivity analysis to unobserved confounding simple, accessible and standardized;

2. They can be easily computed from quantities found in a rregression table, thereby enabling readers and reviwers to assess the sensitivity of results they see in print, even if the original authors did not perform sensitivity analyses.

With this in mind, @cinelli:jrssb2019 propose two main  sensitivity statistics for *routine reporting*: (i) the (observed) partial $R^2$ of the treatment with the outcome, $R^2_{Y\sim D \mid {\bf X}}$; and,  the *robustness value*.

<!-- that can more easily and readily convey the sensitivity of a results in the face of unobserved confounding and can be added to regression tables without having to convey an entire contour plot. -->

### The partial R2 of the treatment with the outcome 

Beyond being an effect measure that quantifies how much variation of the outcome the treatment explains, the partial $R^2$ of the treatment with the outcome can also be used to convey how robust the point estimate is to unobserved confounding in an ``extreme scenario.'' More precisely, suppose the unobserved confounder $Z$ explains \emph{all} residual variance of the outcome, that is, $R_{Y\sim Z|D, {\bf X}}~=~1$.  Then, for this confounder to bring the point estimate to zero, it must explain *at least* as much residual variation of the treatment as the residual variation of the outcome that the treatment currently  explains.  In other words, if  $R_{Y\sim Z|D, {\bf X}}~=~1$, then we must have that  $R^2_{D\sim Z|{\bf X}} \geq R^2_{Y\sim D|{\bf X}}$, otherwise this confounder cannot logically account for all the observed association between the treatment and the outcome \citep{cinelli:jrssb2019}.


### The Robustness Value

The second sensitivity statistics proposed in @cinelli:jrssb2019 is the *robustness value*. The robustness value $RV_{q,\alpha}$ quantifies the *minimal* strength of association that the confounder needs to have, *both* with the treatment and with the outcome, so that a confidence interval of level $\alpha$ includes a change of $q\%$ of the current estimated value.

Let $f_q := q|f_{Y\sim D | {\bf X}}|$, where $|f_{Y\sim D | {\bf X}}|$ is the partial *Cohen's f* of the treatment with the outcome multiplied by the percentage reduction $q$ deemed to be problematic.\footnote{Cohen's $f^2$ can be written as $f^2_{Y\sim D | {\bf X}} = R^2_{Y\sim D | {\bf X}}/(1-R^2_{Y\sim D | {\bf X}})$} Also, let $|t^*_{\alpha, \text{df}-1}|$ denote the t-value threshold for a t-test with significance level of $\alpha$ and $\text{df}-1$ degrees of freedom, and define $f^*_{\alpha, \text{df} - 1} := |t^*_{\alpha, \text{df}-1}|/\sqrt{\text{df} -1}$.   Finally,  construct $f_{q, \alpha}$, which "deducts" from $f_{Y\sim D | {\bf X}}$ both the proportion of reduction $q$ of the point estimate and the boundary below which statistical significance is lost at the level of $\alpha$. That is, $f_{q, \alpha} := f_q - f^*_{\alpha,\text{df} - 1}$. We then have that $RV_{q,\alpha}$ is given by [@cinelli:jrssb2019; @cinelli:wp2020],

\noindent 
\begin{align}
RV_{q, \alpha} =\left\{
\arraycolsep=2pt\def\arraystretch{2}
\begin{array}{ll}
\displaystyle 0, & \text{if}~~f_{q, \alpha} < 0 \\
\displaystyle \frac{1}{2}\left(\sqrt{f_{q, \alpha}^4 + 4f_{q, \alpha}^2} - f_{q, \alpha}^2\right), & \text{if}~~ f_{q} < 1/f^*_{\alpha, \text{df}-1}\\
\displaystyle \frac{f_{q}^2 - f^{*2}_{\alpha,\text{df} - 1}}{1+f^2_{q}}, & \text{otherwise}. 
\end{array}\right.
\label{eq:rvt_main}
\end{align}

\noindent Any confounder that explains $RV_{q,\alpha}\%$ of the residual variance *both* of the treatment and of the outcome is sufficiently strong to make the adjusted t-test not reject the null hypothesis $H_0: \tau = (1-q)|\hat{\tau}_{\text{res}}|$ at the $\alpha$ level (or, equivalently, sufficiently strong to make the adjusted $1-\alpha$ confidence interval include $(1-q)|\hat{\tau}_{\text{res}}|$). Likewise, a confounder with both associations lower than $RV_{q, \alpha}$ is not capable of overturning the conclusion of such a test. Setting $\alpha =1$ returns the robustness value for the point estimate. Further details on how to interpret the RV in practice are given in the next sections.


## Bounds on the strength of confounding using observed covariates {#bounds}

Consider a confounder orthogonal to the observed covariates, ie., $Z \perp {\bf X}$,  or, equivalently, consider only the part of $Z$ not linearly explained by ${\bf X}$. Now denote by $X_j$ a specific covariate of the set ${\bf X}$ and define 

\noindent 
\begin{align}
k_D := \frac{R^2_{D\sim Z|{\bf X}_{-j}}}{R^2_{D\sim X_{j}|{\bf X}_{-j}} },  \qquad k_Y := \frac{R^2_{Y \sim Z |{\bf X}_{-j},D}}{R^2_{Y \sim X_{j} |{\bf X}_{-j},D}}.
\end{align}

where ${\bf X}_{-j}$ represents the vector of covariates ${\bf X}$ excluding $X_{j}$. That is, the terms $k_D$ and $k_Y$ represent how strong the confounder $Z$ is relative to observed covariate $X_j$, where "strength" is measured by how much residual variation they explain of the treatment (for $k_D$) and of the outcome (for $k_Y$).
<!-- Note that we will not know $k_D$ or $k_Y$; rather, any assumed values of ($k_D$, $k_Y$) imposes bounds on the resulting confounding and bias.  -->
<!-- Note that $k_D$ is defined as the *relative strength* of the observed covariate $X_j$, when compared to the unobserved confounder $Z$,   in terms of the explanatory power of treatment variation (after adjusting for the remaining covariates ${\bf X}_{-j}$). If the omission of $X_j$ results in a larger mean squared error in the treatment assignment regression than the omission of $Z$ means $k_D \leq 1$. Similar interpretation can be given to $k_Y$, *mutatis mutandis*.!-->
Given $k_D$ and $k_Y$,  we can rewrite the strength of the confounders as \citep{cinelli:jrssb2019},
\begin{align}
R^2_{D\sim Z|{\bf X}} = k_D f^2_{D\sim X_{j}|{\bf X}{-j}}, \qquad R^2_{Y\sim Z|D, {\bf X}} \leq \eta^2 f^2_{Y \sim X_j|{\bf X}_{-j},D} \label{eq:bounds}
\end{align}

\noindent where $\eta$ is a scalar which depends on $k_Y$, $k_D$ and $R^2_{D\sim X_{j}|{\bf X}{-j}}$.
<!-- we should write $\eta$ out somewhere... !-->

These equations allow the investigator to assess the maximum bias that a hypothetical confounder at most ``k times'' as strong as a particular covariate $X_j$ could cause. This can be used to explore the relative strength of confounding necessary for bias to have been problematic and change the research conclusions. Furthermore, when the researcher has domain knowledge to argue that a certain covariate $X_j$ is particularly important in explaining treatment or outcome variation, and that omitted variables cannot explain as much residual variance of $D$ or $Y$ as that observed covariate, these results can be used to set plausible bounds in the total amount of confonding.


## Multiple or non-linear confounders {#multiple}

Finally, suppose that, instead of a single unobserved confounder $Z$, there are *multiple* unobserved confounders ${\bf Z} = [Z_1, Z_2, \dots, Z_k]$. In this case, the regression the investigator wished she had run becomes,

\noindent 
\begin{align}
Y &= \hat{\tau} D + {\bf X} \hat{{\bf \beta}} +  {\bf Z}\hat{{\bf \gamma}} + \hat{\epsilon}_{\text{full}}.  \label{eq:fulleq2}
\end{align}

As @cinelli:jrssb2019 show, the previous results considering a single unobserved confounder are in fact *conservative* when considering the impact of multiple confounders, barring an adjustment in the degrees of freedom of Equation \ref{eq:r2se}. Moreover, since the vector ${\bf Z}$ is arbitrary, this can also acommodate non-linear confounders or even misspecification of the functional form of the observed covariates ${\bf X}$. In other words, to assess the maximum bias that multiple, non-linear confounders could cause in our current estimates, it suffices to think in terms of the \emph{maximum explantory power} that ${\bf Z}$ could have have in the treatment and outcome regressions, as parameterized by $R^2_{D\sim {\bf Z} | {\bf X}}$ and $R^2_{Y\sim {\bf Z} |D, {\bf X}}$.

# sensemakr for R: basic functionality {#r-basic}

In this section we illustrate the basic functionality of \pkg{sensemkar} for \proglang{R}. Given that sensitivity analysis requires contextual knowledge to be properly interpreted, we illustrate these tools with a real example. We use \pkg{sensemakr} to reproduce all results found in Section 5 of @cinelli:jrssb2019, which estimates the effects of exposure to violence on attitudes towards peace, in Darfur. Further details about this application and the data can be found in @hazlett:jcr2019.


##  Violence in Darfur: data and research question {#darfur}

In 2003 and 2004, the Darfurian government orchestrated a horrific campaign of violence against civilians, killing an estimated two hundred thousand people. This application asks whether, on average, being directly injured or maimed in this episode made individuals more likely to feel “vengeful” and unwilling to make peace with those who perpetrated this violence. Or, might those who directly suffered such violence be motivated to see it end, supporting calls for peace?

The `sensemakr` package provides the data required for this example based on a survey among Darfurian refugees in eastern Chad [@hazlett:jcr2019]. To get started we first need to install the package. From within \proglang{R}, the \pkg{sensemakr} package can be installed from the Comprehensive \proglang{R} Archive Network (CRAN).

```{r install_sensemakr, eval = FALSE}
install.packages("sensemakr")
```

Next, after loading the package, the data can be loaded with the command `data("darfur")`.

```{r pkg_data, message=FALSE}
library(sensemakr)
data("darfur")
```

The "treatment" variable of interest is `directlyharmed`, which indicates whether the individual was physically injured or maimed during the attack on her or his village in Darfur. The main outcome of interest is `peacefactor`, an index measuring pro-peace attitudes.  Other covariates in the data include: `village` (a factor variable indicating the original village of the respondent), `female` (a binary indicator of gender), `age`, `herder_dar` (whether they were a herder in Darfur), `farmer_dar` (whether they were a farmer in Darfur), and `past_voted` (whether they report having voted in an earlier election, prior to the conflict). For further details, see `?darfur`.

@hazlett:jcr2019 argues that the purpose of these attacks was to punish civilians from ethnic groups presumed to support the opposition and to kill or drive these groups out so as to reduce this support. Violence against civilians included aerial bombardments by the government as well as assaults by the *Janjaweed*, a pro-government militia.  For this example, suppose a researcher argues that, while some villages were more or less intensively attacked, within village violence was largely indiscriminate. The bombings could not be finely targeted owing to their crudeness, and there were not many reason to target them.  Similarly, the *Janjaweed* had no reason to target certain individuals rather than others, and no information with which to do so---with one major exception: women were targeted and often subjected to sexual violence. 

Given these considerations, this researcher may argue that adjusting for `village` and `female` is sufficient for control of confounding, and run the following linear regression model (in which other pre-treatment covariates, although not necessary for identification, are also included):

```{r darfur_model}
darfur.model <- lm(peacefactor ~ directlyharmed  + village +  female +
                                 age + farmer_dar + herder_dar + 
                                 pastvoted + hhsize_darfur, 
                   data = darfur)
```

This regression model results in the estimates shown in Table \ref{tab:darfur_ols}. According to this model, those who were directly harmed in violence were on average more "pro-peace," not less.

\begin{table}
\centering
```{r stargazer_darfur, echo=FALSE,  results = 'asis'}
stargazer::stargazer(darfur.model, 
                     keep = c("directlyharmed", "female"), 
                     type = "latex", 
                     header = FALSE, 
                     float = F)
```
\caption{OLS results for \texttt{darfur.model}. Due to sapce, only the results for \texttt{directlyharmed} and \texttt{female} are shown.}
\label{tab:darfur_ols}
\end{table}

### The threat of unobserved confounders

The previous estimate requires the assumption of *no unobserved confounders* for unbiasedness. While supported by the claim that there is no targeting of violence within village and gender strata, not all investigators may agree with this account. For example, although the bombing was crude, perhaps bombs were still more likely to hit the center of the village, and those in the center were also likely to hold different attitudes towards peace. Or, it could be the case that the *Janjaweed* observed signals that indicate individual characteristics such as wealth, and targeted using this information. Or, in another vein, an individual's prior political attitudes could have led them to take actions that exposed them to greater risk during the attack. To complicate things, all these factors could interact with each other or otherwise have other non-linear effects.

These concerns suggest that, instead of the previous linear model (`darfur.model`), we should have run a model such as

```{r darfur_complete, eval=FALSE}
darfur.complete.model <- lm(peacefactor ~ directlyharmed  + village +  
                              female + age + farmer_dar + herder_dar + 
                              pastvoted + hhsize_darfur +
                              center*wealth*political_attitudes, 
                            data = darfur)
```

\noindent Where here `center*wealth*political_attitudes` indicates *fully interacted* terms for these three variables. However trying to fit the model `darfur.complete.model` will result in error: none of the variables `center`, `wealth` or `political_attitudes` were measured. 


This naturally leads to the questions:   given an assumption on how strongly omitted variables relate to the treatment and the outcome, how would including them have changed our inferences regarding the coefficient of `directlyharmed`? Or, what is the *minimal strength* that these unobserved confounders (or *all* remaining unobserved confounders) need to be to change our previous conclusions? Additionally, how plausible are such confounders? For instance, considering the special role of gender in targeting, even if variables such as `wealth` remained as confounders, one could argue that it is unreasonable to expect that they explain more of the variation of exposure to violence than does gender. How much bias could a confounder as strong or stronger than `female` cause?  We show next how `sensemakr` answer each of these questions.

<!-- We show next how `sensemakr` can leverage such claims to bound the plausible strength of unobserved variables. -->

<!-- How can we leverage claims regarding the relative importance of the variable `female` to bound the plausible strength of unobserved variables? We show next how to answer those questions using `sensemakr`.  -->


## Violence in Darfur: sensitivity analysis {#darfur_sensitivity}

The main function in \pkg{sensemkar} for \proglang{R} is `sensemakr()`. This function performs the most commonly required sensitivity analyses and return an object of class `sensemakr`, which can then be further explored with the `print`, `summary` and `plot` methods (see details in `?print.sensemakr` and `?plot.sensemakr`). We begin the analysis by applying `sensemakr()` to the original regression model, `darfur.model`.

```{r sesemakr_darfur}
darfur.sensitivity <- sensemakr(model = darfur.model, 
                                treatment = "directlyharmed",
                                benchmark_covariates = "female",
                                kd = 1:3,
                                ky = 1:3, 
                                q = 1,
                                alpha = 0.05, 
                                reduce = TRUE)
```

The arguments of this call are:

- **model**: the `lm` object with the outcome regression. In our case, `darfur.model`.

- **treatment**:  the name of the treatment variable. In our case, `"directlyharmed"`.

- **benchmark_covariates**: the names of covariates that will be used to bound the plausible strength of the unobserved confounders. Here, we put `"female"`, which one could argue to be among the main determinants of exposure to violence.  It also found to be among the strongest determinants of attitudes towards peace empirically here.

- **kd** and **ky**: these arguments parameterize how many times stronger the confounder is related to the treatment ( `kd` ) and to the outcome ( `ky` ) in comparison to the observed benchmark covariate ( `"female"` ). In our example, setting `kd = 1:3` and `ky = 1:3` means we want to investigate the maximum strength of a confounder once, twice, or three times as strong as female (in explaining treatment and outcome variation).  If only `kd` is given, `ky` will be set equal to it by default.

- **q**: this allows the user to specify what fraction of the effect estimate would have to be explained away to be problematic.  Setting `q = 1`, as we do here, means that a reduction of 100% of the current effect estimate, that is, a true effect of *zero*, would be deemed problematic. The default is `q = 1`.

- **alpha**: significance level of interest for making statistical inferences. The default is set to `alpha = 0.05`.

- **reduce**:  should we consider confounders acting towards *increasing* or *reducing* the absolute value of the estimate? The default is `reduce = TRUE`, which means we are considering confounders that pull the estimate towards (or through) zero. Setting `reduce = FALSE` will consider confunders that pull the estimate *away* from zero.

Using the default arguments, one can simplify the previous call to

```{r sensemakr_darfur_defaults}
darfur.sensitivity <- sensemakr(model = darfur.model, 
                                treatment = "directlyharmed",
                                benchmark_covariates = "female",
                                kd = 1:3)
```

After running `sensemakr()`, we can explore the sensitivity analysis results. We note that the function \texttt{sensemakr()} also has \texttt{formula} and \texttt{numeric} methods. See \texttt{?sensemakr} for details.

### Sensitivity statistics for routine reporting

The print method for `sensemakr` provides the original (unadjusted) estimate along with three summary sensitivity statistics suited for *routine reporting*: (1) the partial $R^2$ of the treatment with the outcome; (2) the robustness value (RV) required to reduce the estimate entirely to zero (i.e. $q=1$); and, (3) the RV beyond which the estimate would no longer be statistically distinguishable from zero at the 5% level ($q=1$, $\alpha=0.05$). 

```{r darfur_print}
darfur.sensitivity
```

The package also provides a function that creates a latex or html table with these results, as shown in Table \ref{tab:minimal} (for the html table, simply change the argument to `format = "html"`). 


```{r latex_table_call, eval = FALSE}
ovb_minimal_reporting(darfur.sensitivity, format = "latex")
```

\begin{table}
\centering
\begin{tabular}{lrrrrrr}
\multicolumn{7}{c}{Outcome: \textit{peacefactor}} \\
\hline \hline 
Treatment: & Est. & S.E. & t-value & $R^2_{Y \sim D |{\bf X}}$ & $RV_{q = 1}$ & $RV_{q = 1, \alpha = 0.05}$  \\ 
\hline 
\textit{directlyharmed} & 0.097 & 0.023 & 4.184 & 2.2\% & 13.9\% & 7.6\% \\ 
\hline 
df = 783 & & \multicolumn{5}{r}{ \small \textit{Bound (1x female)}: $R^2_{Y\sim Z| {\bf X}, D}$ = 12.5\%, $R^2_{D\sim Z| {\bf X} }$ = 0.9\%} \\
\end{tabular}
\caption{Minimal sensitivity analysis reporting.}
\label{tab:minimal}
\end{table}

These three sensitivity statistics provide a  *minimal reporting* style for sensitivity analysis. More precisely:

- The robustness value for bringing the point estimate of `directlyharmed` exactly to zero ($RV_{q=1}$) is 13.9%. This means that unobserved confounders that explain 13.9% of the residual variance *both* of the treatment and of the outcome are sufficiently strong to explain away all the observed effect. On the other hand, unobserved confounders that *do not* explain at least 13.9% of the residual variance *both* of the treatment and of the outcome are not sufficiently strong to do so.

- The robustness value for testing the null hypothesis that the coefficient of `directlyharmed` is zero $(RV_{q =1, \alpha = 0.05})$ falls to 7.6%.  This means that unobserved confounders that explain 7.6% of the residual variance  *both* of the treatment and of the outcome are sufficiently strong to bring the lower bound of the confidence interval to zero (at the chosen significance level of 5%). On the other hand,  unobserved confounders that *do not* explain at least 7.6% of the residual variance *both* of the treatment and of the outcome are not sufficiently strong to do so.

- Finally, the partial $R^2$ of `directlyharmed` with `peacefactor` means that, in an *extreme scenario*, in which we assume that unobserved confounders explain *all* of the left out variance of the outcome, these unobserved confounders would need to explain at least 2.2% of the residual variance of the treatment to fully explain away the observed effect.

These are useful quantities that summarize *what we need to know* in order to safely rule out confounders that are deemed to be problematic. Interpreting these values requires domain knowledge about the data generating process. Therefore, we encourage researchers to argue about what are plausible bounds on the maximum explanatory power that unobserved confounders could have in a given application.

Where investigators are unable to offer strong arguments limiting the *absolute strength* of confounding, it is often informative to make *relative claims*, for instance, by arguing that unobserved confounders are likely not multiple times stronger than a certain observed covariate. In  our application, this is indeed the case. One could argue that, given the nature of the attacks, it is hard to imagine that  unobserved confounding could explain much more of the residual variance of targeting than what is explained by the observed variable `female`. The lower corner of the table, thus, provides bounds on confounding as strong as female, $R^2_{Y\sim Z| {\bf X}, D}$ = 12.5\%, and $R^2_{D\sim Z| {\bf X} }$ = 0.9\%. Since both of those are below the RV, the table reveals that confounders as strong as `female` are not sufficient to explain away the observed estimate. Moreover, the bound on $R^2_{D\sim Z| {\bf X} }$ is below the partial $R^2$ of the treatment with the outcome, $R^2_{Y \sim D |{\bf X}}$. This means that even an extreme confounder explaining *all* residual variation of the outcome and as strongly associated with the treatment as `female` would not overturn the research conclusions As noted in Section \ref{multiple}, these results are exact for a single unobserved confounder, and conservative for multiple confounders, possibly acting non-linearly. 

Finally, the summary method for `sensemakr` provides an extensive report with verbal descriptions verbal descriptions of all these analyses. Here, for instance, entering the command `summary(darfur.sensitivity)` produces verbose output similar to the text explanations in the last several paragraphs (and thus not reproduced here), so that researchers can directly cite or include such text in their reports.

<!-- For further details, please refer to [Cinelli and Hazlett (2020)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348).  -->

<!-- ```{r summary, results='hide'} -->
<!-- summary(darfur.sensitivity) -->
<!-- ``` -->


### Sensitivity contour plots of point estimates and t-values

The minimal sensitivity reporting of Table \ref{tab:minimal} offers a useful summary of how robust the current estimate is to unobserved confounding. For refining their analyses, researchers can visually explore the whole range of possible estimates that confounders with different strengths could cause. These plots can also represent different bounds on the plausible strength of confounding based on different assumptions on how they compare to observed covariates.  For these, one can use the plot method for `sensemakr`. 

We begin by examining the default plot type, contour plots for the point estimate.

```{r estimate_plot_call, eval = FALSE}
plot(darfur.sensitivity)
```

The resulting plot is shown in the left of Figure \ref{fig:darfur_contours}. The horizontal axis shows the hypothetical residual share of variation of the treatment that unobserved confounding explains, $R^2_{D\sim Z| {\bf X} }$. The vertical axis shows the hypothetical partial $R^2$ of unobserved confouding with the outcome, $R^2_{Y\sim Z| {\bf X}, D}$. The contours show what would be the estimate for `directlyharmed` that one would have obtained in the full regression model including unobserved confounders with such hypothetical strengths. Note the plot is parameterized in way that hurts our preferred hypothesis, by pulling the estimate towards zero. Recall that the direction of the bias was set in the argument `reduce = TRUE` of `sensemakr()`.

The bounds on the strength of confounding, determined by the parameter `kd = 1:3` in the call for `sensemakr()`, are also shown in the plot. The plot reveals that the direction of the effect (positive) is robust to confounding once, twice or even three times as strong as the observed covariate `female`, although in this last case the magnitude of the effect is reduced to a third of the original estimate.

We now examine the sensitivity of the *t-value* for testing the null hypothesis of zero effect. For this, it suffices to change the option `sensitivity.of = "t-value"`.

```{r t_plot_call, eval = FALSE}
plot(darfur.sensitivity, sensitivity.of = "t-value")
```

```{r fig_cap_contour, echo=FALSE}
fig.cap <- "\\label{fig:darfur_contours}Sensitivity contour plots of point estimate (left) and t-value (right)"
```

```{r both_plots_real, echo = FALSE, fig.width=8, fig.height=4, out.width='420px',  out.height='210px', fig.cap=fig.cap}
# make plots appear together
old.par <- par(mfrow = c(1,2))
plot(darfur.sensitivity)
plot(darfur.sensitivity, sensitivity.of = "t-value")
par(old.par)
```

The resulting plot is shown in the right of Figure \ref{fig:darfur_contours}. At the 5% significance level, the null hypothesis of zero effect would still be rejected given confounders once or twice as strong as `female`. However, while the point-estimate remains positive, accounting for sampling uncertainty now means that the null hypothesis of zero effect *would not* be rejected with the inclusion of a confounder three times as strong as `female`. 

### Sensitivity plots of extreme scenarios

Sometimes researchers may be better equipped to make plausibility judgments about the strength of determinants of the treatment assignment mechanism, and have less knowledge about the determinants of the outcome. In those cases, sensitivity plots using *extreme scenarios* are a useful option. These are produced with the option `type = extreme`. Here one assumes confounding explains **all** or  some large fraction of the residual variance of the outcome, then vary how strongly such confounding is hypothetically related to the treatment, to see how this affects the resulting point estimate.

```{r fig_cap_extreme, echo = FALSE}
fig.cap.extreme <- "\\label{fig:extreme}Sensitivity analysis to extreme scenarios."
```


```{r darfur_extreme, fig.width=6, out.width="350px", fig.cap=fig.cap.extreme ,fig.pos="t"}
plot(darfur.sensitivity, type = "extreme")
```

Figure \ref{fig:extreme} shows the result. By default these plots consider confounding that explains 100%, 75%, and 50% of variation in the residual outcome,producing three separate curves for each scenario. This is equivalent to setting the argument `r2yz.dx = c(1, .75, .5)`. The bounds on the strength of association of a confounder once, twice or three times as strongly associated with the treatment as `female` are shown as red ticks in the horizontal axis.  As the plot shows, even in the most extreme case ($R^2_{Y\sim Z| {\bf X}, D}=100\%$), confounders would need to be more than twice as strongly associated with the treatment as `female` to fully explain away the point estimate. Moving to  the scenarios $R^2_{Y\sim Z| {\bf X}, D}=75\%$  and  $R^2_{Y\sim Z| {\bf X}, D}=50\%$, confounders would need to be more than three times as strongly associated with the treatment as `female` to fully explain away the point estimate. 


## A disciplined discussion about confounding {#lecturing}

Having demonstrated the basic functionality of the package, we end this section by recalling some important caveats on intepretation that apply to any sensitivity analyses. Readers may refer to @cinelli:jrssb2019 for further discussion. The results computed by `sensemakr()` tell us what we need to be prepared to believe in order to sustain that a given conclusion is not due to confounding. In particular, the results of the sensitivity analysis performed here show that, to explain all the observed estimated effect, even in a worst case scenario where the unobserved confounder explains all residual variation of the outcome, this unobserved confounder would need to be more than twice as strongly associated with the treatment as the covariate `female`. This is a *true quantitative statement* that describes the strength of confounding needed to overturn the research conclusions.  

To counter a common misconception, we emphasize that such an analysis says nothing about whether such a confounder does or does not exist. The role of sensitivity analysis is, instead, to *discipline the discussion* regarding the causal interpretation of the effect estimate. In particular,

1. A causal interpretation of the research conclusion may be defended by articulating that a confounder with such strength is unlikely to exist. For instance, one could argue that, given the way injuries (the "treatment") occurred, the scope for targeting particular types of individuals was quite limited; aircraft dropped makeshift and unguided bombs and other objects over villages, and militia raided without concern for who they would attack---the only known major exception to this, due to sexual assaults, was targeting gender, which is also one of the most visually apparent characteristics of an individual.

2. Likewise, similar grounds are required to persuasively dismiss a causal interpretation of the result. There are requirements, in terms of (relative) strength, that the hypothesized unobserved confounders need to meet in order to be problematic. For instance, helpful skepticism must articulate why a confounder that explains at least more than twice of the variation of the treatment assignment than the covariate `female` is plausible. Otherwise, the putative confounder cannot logically account for all the observed association, even in an extreme scenario.

Robustness to confounding is thus claimed to the extent one agrees with the arguments in 1 (which rely on domain knowledge about attacks in Darfur), while a result can be deemed fragile insofar as alternative stories meeting the requirements in 2 can be offered. Sensitivity analyses should not be used to obviate discussions about confouding by engaging in automatic procedures; rather, they should stimulate a more disciplined, quantitative argument about confounding, in which such statements are made and debated.

# sensemakr for R: advanced use {#r-adv}

The functionality demonstrated in the previous section will suffice for most users, most of the time. Sometimes, however, more flexibility will be needed, and can be obtained by employing additional functions of \pkg{sensemakr} for \proglang{R}. The functions most likely to be called upon can be divided into the following categories: 
<!--We demonstrate several such cases here.

# Customized sensitivity analyses 

Users that want to perform customized sensitivity analyses, beyond what is offered by default in `sensemakr()`, can do so by using the individual functions of the package. These functions can be divided in the following categories, according to their functionalities:
!-->

- *functions for computing the bias, adjusted estimates and standard errors:* these comprise, among others, the functions `bias()`,  `adjusted_estimate()`, `adjusted_se()` and `adjusted_t()`. They take as input the original (unadjusted) estimate (in the form of a linear model or numeric values) and a pair of sensitivity parameters (the partial $R^2$ of the omitted variable with the treatment and the outcome), and return the new quantity adjusted for omitted variable bias. 

- *functions for computing sensitivity statistics*: these comprise, among others, the functions `partial_r2()`,  `robustness_value()`, and `sensitivity_stats()`. These functions compute sensitivity statistics suited for routine reporting, as proposed in @cinelli:jrssb2019. They take as input the original (unadjusted) estimate (in the form of a linear model or numeric values), and return the corresponding sensitivity statistic.

- *sensitivity plots*: These functions provide direct access to sensitivity contour plots, `ovb_contour_plot()`, and sensitivity plots of extreme scenarios, `ovb_extreme_plot()`, for customization. There is also a convenience function `add_bound_to_contour()` which facilitates the placement of user computed bounds on contour plots. All plot functions return invisibly the data needed to replicate the plot, so users that prefer fully customized plots can also easily do so. The default options for plots work best with width and height around 4 to 5 inches.

- *bounding functions*: these functions compute bounds on the strength of confounding "k times" as strong as certain observed covariates. The main high level function is `ovb_bounds()`, and there is also the auxiliary function `ovb_partial_r2_bound()`.

We demonstrate the use of these functions below through examples chosen to illustrate important features of sensitivity analysis.

## Formal versus informal benchmarking: customizing bounds {#informal}

<!-- ### Background -->

Informal “benchmarking” procedures have been suggested as aids to interpretation for numerous sensitivy analyses. These approaches are usually described as revealing how an unobserved confounder $Z$ “not unlike” some observed covariate $X_j$ would alter the results of a study [@imbens2003sensitivity;@blackwell2013selection;@hosman2010sensitivity; @carnegie:jree2016; @dorie2016flexible; @hong:jebs2018]. As shown in \cite{cinelli:jrssb2019}, these informal proposals may lead users to erroneous conclusions, even when they have correct knowledge of how unobserved confounders compare to observed covariates.  Here we replicate Section 6.1 of @cinelli:jrssb2019 using \pkg{sensemakr}, and provide a numerical example that illustrates the potential for misleading results from informal benchmarking. This example also demonstrates advanced usage of the package, including how to construct sensitivity contour plots with customized bounds.

###  Data and model

We begin by simulating the data generating process which will be used in our example, as given by Equations \ref{eq:naive_first} to \ref{eq:naive_last} below. Here we have a treatment variable $D$, an outcome variable $Y$, one observed confounder $X$, and one *unobserved* confounder $Z$. All disturbance variables $U$ are standardized mutually independent normals. Note that, in reality, the treatment $D$ has *no causal effect* on the outcome $Y$. 

\paragraph{Model 1:}
\begin{align}
Z &= U_{z}\label{eq:naive_first}\\
X &= U_{x}\\
D &= X + Z + U_d\\
Y &= X + Z + U_y \label{eq:naive_last}
\end{align}

Also note that, in this model: (i) the unobserved confounder $Z$ is independent of $X$; and, (ii) the unobserved confounder $Z$ is *exactly like* $X$ in terms of its strength of association with the treatment and the outcome. The code below creates a sample of size 100 of this data generating process. We use the function `resid_maker()` to make sure the residuals are standardized and orthogonal, thus all properties that we describe here hold exactly even with finite sample size.

```{r resid_maker,echo=FALSE}
# function for orthogonalizing and
# standardizing residuals
resid_maker <- function(n, C){
  e <- resid(lm(rnorm(n) ~ C))
  e  <- c(scale(e))
  return(e)
}
```

```{r naive_dgp}
n <- 100
X <- scale(rnorm(n))
Z <- resid_maker(n, X) 
D <- X + Z + resid_maker(n, cbind(X, Z)) 
Y <- X + Z + resid_maker(n, cbind(X, Z, D))
```

In this example, the investigator knows she need to adjust for the confounder $Z$ but, unfortunately, does not observe $Z$. Therefore, she is forced to fit the restricted linear model adjusting for $X$ only. 

```{r bench_lm}
model.ydx <- lm(Y ~ D + X) 
```
Results from this regression are shown in the first column of Table \ref{tab:naive}, showing a large and statistically significant coefficient estimate $X$. 

\begin{table}
\centering

```{r stargazer_bench, echo=FALSE, results='asis'}
model.ydxz <- lm(Y ~ D +X + Z)
stargazer::stargazer(model.ydx, model.ydxz,
                     header = FALSE, 
                     keep = c("D","X", "Z"),
                     column.labels = c("Restricted OLS", "Full OLS"),
                     float = FALSE)
```
\caption{First column: results of the restricted regression adusting for $X$ only. Second column: results of the full regression adusting for $X$ and $Z$.}
\label{tab:naive}
\end{table}

<!-- We now demonstrate how functions within `sensemakr` can be used directly to construct benchmark that aid in interpreting sensitivity results from this regression. <!-- How can benchmarking be used to aid in understanding how sensitivity this result is due to the omission of $Z$?!-->

### Formal benchmarks

Let us suppose the investigator *correctly* knows that: (i) $Z$ and $X$ have the same strength of association with $D$ and $Y$; and, (ii) $Z$ is independent of $X$. How can she leverage this information to understand how much bias a confounder $Z$ "not unlike" $X$ could cause? As we have seen in Section \ref{bounds}, using Equation \ref{eq:bounds} it is possible to obtain valid bounds on the amount of confounding caused by an unobserved $Z$ as strongly associated with the treatment $D$ and with the outcome $Y$ as the observed covariate $X$.  

Separately from the main `sensemakr()` function, these bounds can be computed with the function `ovb_bounds()`. In this function one needs to specify the linear model being used (`model = model.ydx`), the treatment of interest (`treatment = "D"`), the observed variable used for benchmarking (`benchmark_covariates = "X"`), and how many times stronger $Z$ is in explaining treatment (`kd = 1`) and outcome (`ky = 1`) variation, as compared to the benchmark variable $X$.

```{r formal_bound}
formal_bound <- ovb_bounds(model = model.ydx, 
                           treatment = "D", 
                           benchmark_covariates = "X", 
                           kd = 1, 
                           ky = 1)
```

We can now inspect the output of `ovb_bounds()`.

```{r formal_bound_no_print, eval = FALSE}
formal_bound[1:6]
```
```{r formal_bound_print, echo = FALSE}
numeric <- sapply(formal_bound, is.numeric)
formal_bound[numeric] <- round(formal_bound[numeric], 3)
rownames(formal_bound) <- NULL
formal_bound[1:6]
```

As we can see, the results of the bounding procedure correctly shows us that, an unobserved confounder $Z$ that is truly "not unlike $X$"  would: (1) explain 50% of the residual variation of the treatment and 33% of the residual variation of the outcome; (2) bring the point estimate exactly to zero; and, (3) bring the standard error to 0.102.  This is precisely what one obtains when running the full regression model adjusting for \emph{both} $X$ and $Z$, as shown in the second column of Table \ref{tab:naive}.

### Informal benchmarks

We now demonstrate an ``informal benchmark'' to show its dangers. Computing the bias due to the omission of $Z$ requires two sensitivity parameters: its partial $R^2$ with the treatment $D$ and its partial $R^2$ with the outcome $Y$.  Informal approaches follow from the intuition that we can simply take the observed partial $R^2$ of $X$ with $D$ and $Y$ found directly from regressions for the treatment and the outcome, respectively. Unfortunately, as formalized in \cite{cinelli:jrssb2019}, these observed associations are themselves affected by the omission of the omitted variable, making naive comparisons potentially misleading. 

What happens if we nevertheless attempt to use those observed statistics for benchmarking? To compute the informal benchmarks, we first need to obtain the observed partial $R^2$ of $X$ with the outcome $Y$. This can be done using the `partial_r2()` function of `sensemakr` in the `model.ydx` regression.

```{r partial_r2_y}
r2yx.d <- partial_r2(model.ydx, covariates = "X")

```

We next need to obtain the partial $R^2$ of $X$ with the treatment $D$. For that, we need to fit a new regression of the treatment $D$ on the observed covariate $X$ here denoted by `model.dx`.

```{r partial_r2_d}
model.dx <- lm(D ~ X)
r2dx   <- partial_r2(model.dx, covariates = "X")
```

We then determine what would be the implied adjusted estimate due to an unobserved confounder $Z$ with this pair of partial $R^2$ values. This can be computed using the `adjusted_estimate()` function. 

```{r informal_adj_estimate}
informal_adjusted_estimate <- adjusted_estimate(model     = model.ydx, 
                                                treatment = "D", 
                                                r2dz.x    = r2dx, 
                                                r2yz.dx   = r2yx.d)
```

Let us now compare those informal benchmarks with the formal bounds. To prepare, we first plot sensitivity contours with the function `ovb_contour_plot()`. Next, we add the informal benchmark to the contours, using the numeric method of the function `add_bound_to_contour()`. Finally, we  use `add_bound_to_contour()` again to add the previously computed formal bounds. 

```{r fig_cap_naive, echo=FALSE}
fig.cap.naive <- fig.cap <- "\\label{fig:naive_benchmarking}Informal benchmarking \\emph{versus} proper bounds."
```

```{r informal_benchmark_plot, fig.align='center', fig.cap = fig.cap.naive}
# draws sensitivity contours
ovb_contour_plot(model = model.ydx,  
                 treatment = "D", 
                 lim = .6)

# adds informal benchmark 
add_bound_to_contour(r2dz.x = r2dx, 
                     r2yz.dx = r2yx.d, 
                     bound_value = informal_adjusted_estimate,
                     bound_label = "Informal benchmark")

# adds formal bound
add_bound_to_contour(bounds = formal_bound, 
                     bound_label = "Formal bound")
```

Note how the results from informal benchmarking are misleading: even though $Z$ and $X$ are exactly alike in their roles,  the informal benchmark point is still far from zero, which would suggest that an unobserved confounder $Z$ "not unlike" $X$ is unable to explain away the observed effect, when in fact we know it would, as shown in Table \ref{tab:naive}. This incorrect conclusion occurs *despite* the investigator *correctly* assuming both that: (i) $Z$ and $X$ have the same strength of association with $D$ and $Y$; and, (ii) $Z$ is independent of $X$.  Therefore, we do not recommend using informal benchmarks for sensitivity analysis, and suggest empirical researchers to use formal approaches such as the ones provided with `ovb_bounds()`. For further details and discussion, we point readers to Sections 4.4 and 6.1 of @cinelli:jrssb2019.


## Assessing the sensitivity of existing regression results {#print}

We conclude this section by demonstrating how to replicate Section \ref{r-basic} using only the statistics found in the regression table along with the individual functions available in the package.

### Sensitivity statistics

The robustness value and partial $R^2$ are key sensitivity statistics, useful for standardized sensitivity analyses reporting. Beyond the main `sensemakr()` function, these statistics can be computed directly by the user with the functions \code{robustness_value()} and \code{partial_r2()}. With a fitted \code{lm} model in hand, the most convenient way to compute the RV and partial $R^2$ is by employing the \code{lm} methods for these functions, as in

```{r rv_lm, results='hide'}
robustness_value(model = darfur.model, covariates = "directlyharmed")
partial_r2(model = darfur.model, covariates = "directlyharmed")
```

\noindent However, when one does not have access to the data in order to run this model, simple summary statistics such as: (i) the point estimate for the `directlyharmed`  (0.097); (ii) its estimtaed standard error (`0.023`); and, (ii) the degrees of freedom of the regression (`783`) are sufficient to compute the RV and partial $R^2$.

```{r rv_num, results='hide'}
robustness_value(t_statistic = 0.097/0.023, dof = 783)
partial_r2(t_statistic = 0.097/0.023, dof = 783)
```

The convenience function \code{sensitivity_stats()} also computes all sensitivity statistics for a regression coefficient of interest and returns them in a \code{data.frame}.

### Plotting functions

All plotting functions can be called directly with `lm` objects or numerical data. For example, the code below uses the function `ovb_contour_plot()` to replicate Figure \ref{fig:darfur_contours} (without the bounds) using only the summary statistics of Table \ref{tab:darfur_ols}.

```{r plot_numeric, eval = FALSE}
ovb_contour_plot(estimate = 0.097, se = 0.023, dof = 783)
ovb_contour_plot(estimate = 0.097, se = 0.023, dof = 783, 
                 sensitivity.of = "t-value")
```

The extreme scenario plots (as in Figure \ref{fig:extreme}) can also be reproduced from summary statistics using the function `ovb_extreme_plot()`,

```{r extreme_numeric, eval = FALSE}
ovb_extreme_plot(estimate = 0.097, se = 0.023, dof = 783)
```

All plotting functions return (invisibly) the data needed to reproduce them, allowing users to create their own plots if they prefer. 

### Adjusted estimates, standard errors and t-values 

These functions allow users to compute the adjusted estimates given different postulated degrees of confounding. For instance, suppose a researcher has reasons to believe a confounder explains 10% of the residual variance of the treatment and 15% of the residual variance of the outcome. If the underlying data are not available, the investigator can still compute the adjustd estimate and t-value that one would have obtained in the full regression adjusting for such confounder.

```{r adj_est_t}
adjusted_estimate(estimate = 0.097, se = 0.023, dof = 783, 
                  r2dz.x = .1, r2yz.dx = 0.15)
adjusted_t(estimate = 0.097, se = 0.023, dof = 783, 
           r2dz.x = .1, r2yz.dx = 0.15)
```

The computation shows that this confounder is not strong enough to bring the estimate to zero, but it is sufficient to bring the t-value below the usual 5% signifiance threshold of 1.96.

### Computing bounds from summary statistics

Finally, we show how users can compute bounds on the strength of confounding using only summary statistics, if the paper also provides a *treatment* regression table, i.e., a regression of the treatment on the observed covariates. Such regressions are sometimes shown in published works as part of efforts to describe the "determinants" of the treatment, or as "balance tests" in which the investigator assesses whether observed covariates predict treatment assigment. For the Darfur example, this regression is shown in Table \ref{tab:treat_reg}.

\begin{table}
\centering
```{r stargazer_treat_darfur, echo=FALSE,  results = 'asis'}
treat.model <- lm(directlyharmed ~ village +  female +
                                 age + farmer_dar + herder_dar + 
                                 pastvoted + hhsize_darfur, 
                   data = darfur)
stargazer::stargazer(treat.model, 
                     keep = c("female"), 
                     type = "latex", 
                     header = FALSE, 
                     float = F)
```
\caption{Treatment regression for the Darfur example. Due to space, only the results for \texttt{female} are shown, which will be used for benchmarking.}
\label{tab:treat_reg}
\end{table}

Using the results of Tables \ref{tab:darfur_ols} and \ref{tab:treat_reg} we can compute the bounds on confounding 1, 2 and 3 times as strong as `female`, as we have done before. First we compute the partial $R^2$ of `female` with the treatment and the outcome

```{r r2_female}
r2yxj.dx <- partial_r2(t_statistic = -0.232/0.024, dof = 783)
r2dxj.x <- partial_r2(t_statistic = -0.097/0.036, dof = 783)
```

Next, we compute the bounds on the partial $R^2$ of the unobserved confounder using the `ovb_partial_r2_bound()` function.

```{r manual_bounds}
bounds <- ovb_partial_r2_bound(r2dxj.x = r2dxj.x,
                               r2yxj.dx = r2yxj.dx,
                               kd = 1:3,
                               ky = 1:3,
                               bound_label = paste(1:3, "x", "female"))
```

Finally, the `adjusted_estimate()` function computes the estimates implied by such hypothetical confounders.

```{r man_adju_est}
bound.values <- adjusted_estimate(estimate = 0.0973,
                                  se = 0.0232,
                                  dof = 783,
                                  r2dz.x = bounds$r2dz.x,
                                  r2yz.dx = bounds$r2yz.dx)

```

This information along with the numeric methods for the plot functions, allow us to reproduce the contour plots of Figure \ref{fig:darfur_contours} using only summary statistics. Note that, since we are performing all calculations manually, appropriate limits of the plot area need to be set by the user.

```{r manual_contour_plot_bound, eval=FALSE}
ovb_contour_plot(estimate = 0.0973, se = 0.0232, dof = 783, lim = 0.45)
add_bound_to_contour(bounds, bound_value = bound.values)
```

# sensemakr for Stata {#stata}

For \proglang{Stata} users, we have also developed a homonymous package \pkg{sensemakr}. 

\pkg{sensemakr} for \proglang{Stata] is available for download on SCC...

It also comes with the `darfur` data (?).

In this section, we demonstrate how to replicate the analysis of Section \ref{r-basic} using the \proglang{Stata] implementation of sensemakr. The main function of the \pkg{Stata} package is `sensemakr`...





# Final Remarks

In this article we have demonstrated how to perform sensitivity analysis to unobserved confounding using the \proglang{R} and \proglang{Stata} packages \pkg{sensemakr}. 


These packages can help making sensitivity analyisis routine and standardized.




# References
