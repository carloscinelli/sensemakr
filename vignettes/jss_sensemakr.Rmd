---
documentclass: jss
author:
  - name: Carlos Cinelli 
    affiliation: University of California, Los Angeles
    address: >
      Department of Statistics,
      8125 Math Sciences Building,
      Los Angeles, CA 90095, USA.
    email: \email{carloscinelli@ucla.edu}
    url: http://carloscinelli.com
  - name: Jeremy Ferwerda
    affiliation: Dartmouth College
    address: >
      Department of Government, 
      Hanover, NH 03755
    email: \email{jeremy.a.ferwerda@dartmouth.edu}
    url: http://jeremyferwerda.com/

  - name: Chad Hazlett
    affiliation: University of California, Los Angeles
    address: >
      Department of Statistics, 
      8125 Math Sciences Building, 
      Los Angeles, CA 90095, USA.
    email: \email{chazlett@ucla.edu}
    url: http://chadhazlett.com
title:
  # If you use tex in the formatted title, also supply version without
  # For running headers, if needed
  formatted: "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
  plain:     "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
  short:     "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
abstract: "This paper introduces the the \\proglang{R} package \\pkg{sensemakr} for assesing the sensitivity of regression estimates to unobserved confounding. The package provides a suite of tools for sensitivity analysis in regression models developed in @cinelli:jrssb2019. These tools are based on the familiar omitted variable bias framework, and can be easily computed using only standard regression results. Furthermore, they do not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handle multiple confounders, possibly acting non-linearly, and enable bounding of sensitivity parameters employing domain knowledge."
keywords:
  # at least one keyword must be supplied
  formatted: [causal inference, sensitivity analysis, omitted variable bias, robustness value]
  plain:     [causal inference, sensitivity analysis, omitted variable bias, robustness value]
preamble: >
  \usepackage{amsmath}
output: 
  rticles::jss_article:
    fig_caption: yes
bibliography: sensemakr.bib
biblio-style: jss      #Added Citation style is listed to use in JSS Instructions for Authors.
editor_options: 
  chunk_output_type: console
graphics: yes
---

```{r setup, include=FALSE}
library(knitr)
library(stargazer)

rm(list = ls())

# Set default plot sizes
## - fig.width and fig.heights are R's plot sizes,
## The default options of contours 
## works best with heigh and width around 4 to 5
## - out.width and out.height just resize the image
## 250px seems optimal, and takes less than half a page.
## for extreme plots, change manually fig.width to 6 and out.width to 350px
opts_chunk$set(fig.width  = 4.5, 
               fig.height = 4.5, 
               fig.pos    = "!tp",
               message    = FALSE, 
               out.width  = '250px', 
               out.height = '250px',
               fig.align  = "center")

opts_chunk$set(tidy = FALSE)

hook_output = knit_hooks$get('output')

knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  n <- options$linewidth
  if (!is.null(n)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```



# Introduction

<!-- Investigating the causal effect of some variable (the "treatment") on another (the "outcome") using observational data poses a perenial problem for researchers across disciplines.  -->
The most common strategy for making causal inferences with observational data is to adjust for observed covariates, and the most common method for doing such adjustment is  (linear) regression. Claiming that the resulting regression coefficient is an unbiased estimate of the causal effect of interest, however, relies on the untestable assumption of no unobserved confounders. Rarely can this assumption be expected to hold exactly, and \emph{qualitative} debates of whether or not \emph{any} unobserved confounding exists are generally not productive---it always does. Therefore, a more useful discussion is \emph{quantitative}: how strong would unobserved confounding need to be to change the research conclusions? Are confounders with such strength plausible? Sensitivity analysis allows us to quantitatively discuss the fragility of putative causal estimates when the underlying assumption of no unobserved confounding is challenged.

The goal of this paper is to introduce the R package \pkg{sensemakr}, whcih implements a suite of tools proposed in @cinelli:jrssb2019 for assessing the sensitivity of a regression coefficient to the inclusion of omitted variables. The goal of \pkg{sensemakr} is to make it easy to understand the impact that omitted variables would have on a regression result. This allows analysts to investigate the robustness of their estimates to violations of the assumption of no unobserved confounding, answering questions such as:

- How strong would an unobserved confounder (or a group of confounders) have to be to change our research conclusions?

- In a worst-case scenario, how robust are our results to all unobserved confounders acting together, possibly non-linearly?

- How strong would confounging need to be relative to the strength of observed covariates, to change our answer a certain amount?



<!-- Weaker claims, e.g. that the causal effect has a particular sign, similarly require the untestable assumption that unobserved confounding does not exceed a certain level. Sensitivity analyses aid in evaluating such claims, by equiping investigators with information about how sensitive a given result is to varying degrees of unobserved confounding, thereby allowing a quantitative discussion regarding the fragility of putative causal estimates when the underlying assumption of no unobserved confounding is challenged. -->

<!-- improving upon our ability to understand and transparently communicate results under confounding. -->


<!-- In the common setting where researchers "adjust for observables" using tools such as regression,  claiming that the resulting estimate is unbiased for a causal quantity rely on untestable assumptions about the absence of unobserved confounders. Weaker claims, e.g. that the causal quantity has a particular sign, similarly require that unobserved confounding does not exceed a certain level. Sensitivity analyses aid in evaluating such claims, by equiping investigators with information about how sensitive a given result is to varying degrees of unobserved confounding, improving upon our ability to understand and transparently communicate results under confounding.  -->


Although several sensitivity analyses have been proposed, dating back to @cornfield1959smoking, with more recent contributions including [@rosenbaum1983assessing; @robins1999association; @frank:smr2000; @rosenbaum2002gamma; @imbens2003sensitivity; @brumback2004sensitivity; @frank:eepa2008; @hosman2010sensitivity; @imai2010identification; @arah2011; @blackwell2013selection; @frank2013would; @carnegie:jree2016; @dorie2016flexible; @middleton2016bias; @oster:jbes2017; @cinelli:icml2019; @franks:jasa2019] Yet, such sensitivity analyses remain underutilized. We argue that a number of factors contribute to this reluctant uptake. One is the complicated nature and strong assumptions many of these methods impose, sometimes involving restrictions on or even a complete description of the nature of the confounder. A second reason is that though users routinely report "regression tables" (or perhaps coefficient plots) to convey the results of a regression, until recently we have lacked "standard" quantities that can simply and correctly summarize sensitivity in the face of unobserved confounding. Third, and most fundamentally, connecting the results of a formal sensitivity analysis to a cogent argument about what types of confounders may exist in one's research project is often difficult, particularly when there are no compelling arguments as to why the treatment assignment should be approximately "ignorable", "exogeneous", or "as-if random".  Further, some of the solutions offered by the literature can lead users to erroneous conclusions.  



<!-- Carlos: I think the quick start ends up not being very useful, because we need to explain everything anyway... -->
<!-- ## Illustration -->

<!-- As a "quick-start" guide, we first show the basic functionality of the package in a simulated example. This functionality will suffice for most users most of the time. The basic workflow is as follows:  -->

<!-- 1) Fit a linear outcome model using `lm.out <- lm()`. This should have your treatment and (pre-treatment) covariates on the right hand side.    -->

<!-- 2) Create a sensemakr object, `sense.out <- sensemakr(lm.out)`, which contains useful sensitivity quantitites. -->

<!-- 3) Explore the results by use of `plot(sense.out)` and `summary(sense.out)`, or through direct calls to the lower-level functions these methods use. -->

<!-- To demonstrate, we first create a simulated dataset. Consider the following linear structural model with a treatment variable $D$, an outcome variable $Y$ and two "confounders" $X$ and $Z$. All disturbance variables $U$ are mutually independent. Note that, in reality, the treatment $D$ has no causal effect on the outcome $Y$.  -->

<!-- \begin{align} -->
<!-- Z &= U_{z}\\ -->
<!-- X &= U_{x}\\ -->
<!-- D &= X + Z + U_d\\ -->
<!-- Y &= X + Z + U_y -->
<!-- \end{align} -->

<!-- The code below creates a sample of size $n = 500$ from this data generating process. -->

<!-- ```{r} -->
<!-- set.seed(10) -->
<!-- n <- 500 -->
<!-- Z <- rnorm(n) -->
<!-- X <- rnorm(n) -->
<!-- D <- X + Z + rnorm(n, sd = 3) -->
<!-- Y <- X + Z + rnorm(n, sd = 3) -->
<!-- ``` -->

<!-- Suppose that an investigator is interested in estimating the causal effect of $D$ on $Y$, but unfortunately the confounder $Z$ is not observed. Despite this, the investigator proceeds with estimation, running a linear regression model adjusting for $X$ only. This results in the following estimates in Table \ref{tab:example}. -->

<!-- ```{r} -->
<!-- lm.model <- lm(Y ~ D + X) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, include = TRUE, results = 'asis'} -->
<!-- stargazer::stargazer(lm.model, header = FALSE,  -->
<!--                      keep = "D",  -->
<!--                      label = "tab:example",  -->
<!--                      title = "restricted OLS results") -->
<!-- ``` -->

<!-- The estimated coefficient for $D$ in the regression model adjusting for $X$ is statistically signficant and large relative to the scale of $D$. The investigator, however, knows that she has not measured all relevant confounders, and that the observed association between $Y$ and $D$ could be due to the omission of some variable $Z$. Further, to aid interpretation or even argue for bounds on confounding, she considers a working assumption that the omitted variable $Z$ is at least as strong as the observed variable $X$, meaning it explains at least as much residual variation in $D$ and $Y$ as $Z$ does.  -->

<!-- How would our estimate vary as we recognize or postulate different degrees of potential confounding? At what point would confounding become strong enough to substantially alter the conclusions of the study? And how can we compare the strength of confounding needed to alter our result to observed covariates to better understand what such confounding implies, or to bound the degree of confounding we think is possible? The \pkg{sensemakr} package provides tools to answer these types of questions. -->

<!-- <!-- Old version: How strong would $Z$ need to be to substantially alter the conclusions of the study? Is that strength plausible? Or, more precisely, how strong would $Z$ need to be relative to the observed covariate $X$? The \pkg{sensemakr} package provides tools to answer these types of questions.!--> -->

<!-- To begin, the investigator passes the estimated model to the function `sensemakr()`, along with the treatment of interest $D$ and optionally an indication of what observed covariates will be used for comparison or "benchmarking", -->
<!-- ```{r, message = FALSE} -->
<!-- library(sensemakr) -->
<!-- sense.model <- sensemakr(model = lm.model,   -->
<!--                          treatment = "D",  -->
<!--                          benchmark_covariates = "X",  -->
<!--                          kd = 1) -->
<!-- sense.model -->
<!-- ``` -->

<!-- The output contains values such as the partial variance in the outcome explained by the treatment, and the "robustness value". For example, the robustness value of 0.14 indicates that confounding explaining less than 14\% of both the treatment and outcome residual variance would not be sufficient to fully account for the estimate. We defer further explanation until below.  Further, full text output in plain English, describing the meaning of these parameters, is provided through `summary(sense.model)`. -->

<!-- The user may also be interested in a number of graphical illustrations of how the estimate varies under hypothesized confounding, including how it would be affected if confounding is "as strong as" the covariate $X$. For example, we can see contour plots of adjusted estimates for the t-statistic, -->


<!-- ```{r, include = FALSE} -->
<!-- cap.example <- "\\label{fig:example} Sensitivity contours of the t-value" -->

<!-- ## plot config -->
<!-- cex.lab = .7 -->
<!-- cex.label.text = .6 -->
<!-- cex.axis = .7 -->
<!-- labcex = .6 -->
<!-- ``` -->

<!-- ```{r, fig.cap= cap.example, fig.pos = "!h", message = FALSE} -->
<!-- plot(sense.model, sensitivity.of = "t-value") -->
<!-- ``` -->


<!-- where the point labeled `1xX` indicates the adjusted estimate as it would appear had confouding been as strong as $X$.  -->


# Sensitivity analysis in an omitted variable bias framework

In this section, we briefly review the omitted variable bias (OVB) framework for sensitivity analysis presented in @cinelli:jrssb2019. This method builds upon a scale-free reparameterization of the OVB formula in terms of partial $R^2$ values. This reparameterization  enables a number of useful analyses, such as: 

- assessing the sensitivity of multiple confounders acting together, possibly non-linearly;
- assessing the sensitivity to extreme scenarios in which all (or a big portion) of the residual variation of the oucome is assumed to be explained by unobserved confounding;
- exploiting knowledge of relative strength of variables to bound the bias due to unobserved confounding;
- presenting all sensitivity results concisely, for easy routine reporting.

Readers familiar with the method may skip to the next sections, in which we show its implementation with \pkg{sensemakr}.

<!-- This approach shows how the familiar “omitted variable bias” (OVB) framework can be extended to address these challenges in the linear regression setting.  -->
<!-- Notably, all these results do not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounder, and can be used to assess the sensitivity to multiple confounders, whether they influence the treatment and outcome linearly or not.  -->


<!-- Readers that want to see these tools applied in practice, may skip this section. -->


<!-- \textcolor{blue}{CJH: I wouldn't mention these yet -- would define bias and give two formulas then describe RV and partial R2yd.} The “robustness value” describes the minimum strength of association unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial $R^2$ of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates, t-values, as well as ``extreme scenarios'' !-->

## The OVB framework

The starting point of our analysis is a *full* linear regression model of an outcome $Y$ on a treatment $D$, controlling for a set of covariates given by \emph{both} ${\bf X}$ and $Z$,
\begin{align}
Y &= \hat{\tau} D + {\bf X} \hat{{\bf \beta}} +  \hat{\gamma}Z + \hat{\epsilon}_{\text{full}}  \label{eq:fulleq}
\end{align}
\noindent where $Y$ is an $(n \times 1)$ vector containing the outcome of interest for each of the $n$ observations and $D$ is an $(n \times 1)$ treatment variable (which may be continuous or binary); ${\bf X}$ is an $(n \times p)$ matrix of \emph{observed} covariates including the constant; and $Z$ is a single $(n \times 1)$ \emph{unobserved} covariate (we discuss how to extend results for a multivariate $Z$ below).  

Equation \ref{eq:fulleq} is the regresion model that the investigator *wished* she had run to obtain a valid causal estimate of the effect of $D$ on $Y$. Nevertheless,  $Z$ is unobserved. Therefore, the feasible regression the investigator is able to estimate is the *restricted* model \emph{omitting} $Z$, that is,
\begin{align}
Y  &= \hat{\tau}_{\text{res}} D + {\bf X} \hat{{\bf \beta}}_{\text{res}} + \hat{\epsilon}_{\text{res}}\label{eq:restrictedeq}
\end{align}

Given the discrepancy of what we wish to know and what we actually have, the main question we would like to answer is: how do the observed point estimate and standard error of the restricted regression, $\hat{\tau}_{\text{res}}$ and $\widehat{se}(\hat{\tau}_{\text{res}})$, compare to the desired point estimate and standard error of the full regression, $\hat{\tau}$ and $\widehat{se}(\hat{\tau})$?

### OVB with the partial R2 parameterization

<!-- \textcolor{blue}{We can take this route -- FWL to show the classical OVB in the coefficient scale, then switch to R2. Or we can just assert the bias is given by one formula for the estimate and one for the SE. In the Colombia paper, we now do the latter, so its very quick. Here though I like what we have now and lean towards keeping it -- its quick enough that the improvement in understanding is worth it, I think.} !-->

Define as $\widehat{\text{bias}}$ the difference between the full and restricted  estimates, 

\begin{align}
\widehat{\text{bias}}~:=~\hat{\tau}_{\text{\text{res}}}~-~\hat{\tau}
\end{align}

Now let: (i) $R^2_{D\sim Z | {\bf X}}$ denote the share of residual variance of the *treatment* explained by the omitted variable $Z$, after accounting for ${\bf X}$; and, (ii)  $R^2_{Y\sim Z|D, {\bf X}}$ denote the share of residual variance of the *outcome* explained by the omitted variable $Z$, after accounting for ${\bf X}$ and $D$. Then, @cinelli:jrssb2019 have shown that the bias can be written as,

\begin{align}
|\widehat{\text{bias}}| &= \text{se}(\hat{\tau}_{\text{res}}) \sqrt{\frac{ R^2_{Y\sim Z|D, {\bf X}}~ R^2_{D\sim Z | {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} (\text{df})} \label{eq:r2bias2}
\end{align}

Where $\text{df}$ stands for the degrees of freedom of the restricted regression actually run. Moreover, the estimated standard error of $\hat{\tau}$ can be recoverd with,

\begin{align}
\text{se}(\hat{\tau})  = \text{se}(\hat{\tau}_{\text{res}}) \sqrt{\frac{1 - R^2_{Y\sim Z|D, {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} \left(\frac{\text{df}}{\text{df}-1}\right)}.  \label{eq:r2se} 
\end{align}

Equations \ref{eq:r2bias2} and \ref{eq:r2se} reveal that, given  hypothetical values of $R^2_{D\sim Z | {\bf X}}$ and  $R^2_{Y\sim Z|D, {\bf X}}$, investigators can examine the sensitivity of  point estimates and standard-errors (and consequently also t-values, confidence intervals or p-values) to the inclusion of any omitted variable $Z$ with such strength. Another useful property of this parameterization is that the effect of $R^2_{Y\sim Z|D, {\bf X}}$ on the bias is bounded. This allows investigators to contemplate extreme sensitivity scenarios, in which the parameter $R^2_{Y\sim Z|D, {\bf X}}$ is set to 1 (or another conservativie value), and see what happens as $R^2_{D\sim Z | {\bf X}}$ varies. 


<!-- One particular interesting way to assess this sensitivity is with the help of *contour plots*, which will be demonstrated with the package  \pkg{sensemakr} in the next section. -->


<!-- Using the Frisch-Waugh-Lovell (FWL) theorem (\citealp{frisch1933partial, lovell1963seasonal, lovell2008simple}) we can  ``partial out'' the observed covariates ${\bf X}$ to obtain the traditional OVB solution, -->

<!-- \begin{align} -->
<!-- \hat{\tau}_{\text{\text{res}}} &= \frac{\text{cov}(D^{\perp {\bf X}}, ~Y^{\perp {\bf X}})}{\text{var}(D^{\perp {\bf X}})} \notag\\ -->
<!-- 			&= \frac{\text{cov}(D^{\perp {\bf X}},~\hat{\tau} D^{\perp {\bf X}} + \hat{\gamma}Z^{\perp {\bf X}} )}{\text{var}(D^{\perp {\bf X}})} \notag\\ -->
<!-- 			&= \hat{\tau} +  \hat{\gamma}\left(\frac{\text{cov}(D^{\perp {\bf X}},~Z^{\perp {\bf X}})}{\text{var}(D^{\perp {\bf X}})} \right) \notag\\ -->
<!-- 				&= \hat{\tau} + \hat{\gamma}\hat{\delta}  \label{eq:biasdef} -->
<!-- \end{align} -->

<!-- \noindent where $\text{cov}(\cdot)$ and $\text{var}(\cdot)$ denote the \emph{sample} covariance and variance; $Y^{\perp {\bf X}}$, $D^{\perp {\bf X}}$ and $Z^{\perp {\bf X}}$ are the variables $Y$, $D$ and $Z$ after removing the components linearly explained by ${\bf X}$ and we define $\hat{\delta}~:=~\frac{\text{cov}(D^{\perp {\bf X}}, Z^{\perp {\bf X}})}{\text{var}(D^{\perp {\bf X}})}$. We then have -->
<!-- \begin{align} -->
<!-- \widehat{\text{bias}} =  \hat{\gamma}\hat{\delta}  \label{eq:deltagamma} -->
<!-- \end{align} -->

<!-- ###  Reparameterizing the OVB formula -->

<!-- Appealing again to the FWL theorem, we can expand Eq. \ref{eq:deltagamma} -->

<!-- \begin{align} -->
<!-- \widehat{\text{bias}} &=  \hat{\delta}\hat{\gamma}\notag\\ -->
<!--  &= \left(\frac{\text{cov}(D^{\perp {\bf X}}, ~ Z^{\perp {\bf X}})}{\text{var}(D^{\perp {\bf X}})}\right) \left(\frac{\text{cov}(Y^{\perp {\bf X},D}, ~Z^{\perp {\bf X},D})}{\text{var}(Z^{\perp {\bf X},D})}\right) \notag\\ -->
<!-- &= \left(\frac{\text{cor}(D^{\perp {\bf X}}, ~ Z^{\perp {\bf X}})\text{sd}(Z^{\perp {\bf X}})}{\text{sd}(D^{\perp {\bf X}})}\right) \left(\frac{\text{cor}(Y^{\perp {\bf X},D}, ~Z^{\perp {\bf X},D})\text{sd}(Y^{\perp {\bf X}, D})}{\text{sd}(Z^{\perp {\bf X},D})}\right) \notag\\ -->
<!-- &=  \left(\frac{\text{cor}(Y^{\perp {\bf X}, D}, ~Z^{\perp {\bf X},D}) \text{cor}(D^{\perp {\bf X}}, ~ Z^{\perp {\bf X}})}{\frac{\text{sd}(Z^{\perp {\bf X}, D})}{\text{sd}(Z^{\perp {\bf X}})}} \right)  \left(\frac{\text{sd}(Y^{\perp {\bf X},D})}{ \text{sd}(D^{\perp {\bf X}}) } \right) \label{eq:biascor} -->
<!-- \end{align} -->

<!-- Noting that $\text{cor}(Y^{\perp {\bf X},D},Z^{\perp {\bf X},D}) ^ 2 = R^2_{Y\sim Z|{\bf X},D}$,  that $\text{cor}(Z^{\perp {\bf X}}, ~D^{\perp {\bf X}})^2 = R^2_{D\sim Z | {\bf X}}$, and that $\frac{\text{var}(Z^{\perp {\bf X}, D})}{\text{var}(Z^{\perp {\bf X}})} = 1-R^2_{Z \sim D | {\bf X}} = 1-R^2_{D \sim Z | {\bf X}}$, we can rewrite Eq. \ref{eq:biascor}to rely only partial $R^2$ measures of association rather than raw regression coefficients, -->

<!-- \begin{align} -->
<!-- |\widehat{\text{bias}}| &= \sqrt{\frac{ R^2_{Y\sim Z|D, {\bf X}}~ R^2_{D\sim Z | {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}}} \left(\frac{\text{sd}(Y^{\perp {\bf X}, D})}{ \text{sd}(D^{\perp {\bf X}}) }\right). \label{eq:r2bias} -->
<!-- \end{align} -->

<!-- Investigators may be interested in how confounders alter inference as well, so we also examine the standard error. Let $\text{df}$ denote the model degrees of freedom (for the restricted regression actually run). Noting that -->

<!-- \begin{align} -->
<!-- \text{se}(\hat{\tau}_{\text{res}}) &= \frac{\text{sd}(Y^{\perp {\bf X},D})}{\text{sd}(D^{\perp {\bf X}})} \sqrt{\frac{1}{\text{df}}} \label{eq:seres} \\   -->
<!-- \text{se}(\hat{\tau}) &= \frac{\text{sd}(Y^{\perp {\bf X},D, Z})}{\text{sd}(D^{\perp {\bf X}, Z})} \sqrt{\frac{1}{\text{df} - 1}}, \label{eq:sefull} -->
<!-- \end{align} -->

<!-- \noindent  whose ratio is -->
<!-- \begin{align} -->
<!-- \frac{\text{se}(\hat{\tau})}{\text{se}(\hat{\tau}_{\text{res}})} &= \left(\frac{\text{sd}(Y^{\perp {\bf X}, D, Z})}{\text{sd}(Y^{\perp {\bf X},D})}  \right)  -->
<!--                                                                  \left(\frac{\text{sd}(D^{\perp {\bf X}})}{\text{sd}(D^{\perp {\bf X}, Z})}  \right)  -->
<!--                                                                  \sqrt{\frac{\text{df}}{\text{df}-1}}, -->
<!-- \end{align} -->

<!-- \noindent  we obtain the expression for the standard error of $\hat{\tau}$ -->

<!-- \begin{align} -->
<!-- \text{se}(\hat{\tau})  = \text{se}(\hat{\tau}_{\text{res}}) \sqrt{\frac{1 - R^2_{Y\sim Z|D, {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} \left(\frac{\text{df}}{\text{df}-1}\right)}.  \label{eq:r2se}  -->
<!-- \end{align} -->

<!-- \noindent Moreover, with this we can further see the bias as -->
<!-- \begin{align} -->
<!-- |\widehat{\text{bias}}| &= \text{se}(\hat{\tau}_{\text{res}}) \sqrt{\frac{ R^2_{Y\sim Z|D, {\bf X}}~ R^2_{D\sim Z | {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} (\text{df})}. \label{eq:r2bias2} -->
<!-- \end{align} -->


<!-- ### Making sense of the partial R2 parameterization -->

<!-- In the partial $R^2$ parameterization, the relative bias, $\left| \frac{\widehat{\text{bias}}}{\hat{\tau}_{\text{res}}}\right|$, has a simple form: -->
<!-- \begin{align} -->
<!-- \text{relative bias}  = -->
<!-- \frac{\overbrace{|R_{Y\sim Z|D, {\bf X}} \times f_{D\sim Z|{\bf X}}|}^\textrm{bias factor}}{\underbrace{|f_{Y\sim D|{\bf X}}|}_\textrm{partial f of D with Y}} = -->
<!-- \frac{\text{BF}}{|f_{Y\sim D|{\bf X}}|}. \label{eq:bf} -->
<!-- \end{align} -->

<!-- where $f_{D\sim Z|{\bf X}}$ is the partial Cohen's $f$.\footnote{Cohen's $f$ is a measure of strength of association which , $R^2/(1-R^2)$. Here we replace that $R^2$ with the relevant partial value, $R^2_{D\sim Z|{\bf X}}$ to produce a Cohen's $f$ that describes how strongly $Z$ predicts $D$ given $\bf{X}$} -->

<!-- How the confounder affects the variance has a straightforward interpretation as well. The relative change in the variance, $\frac{\text{var}(\hat{\tau})}{\text{var}(\hat{\tau}_{\text{res}})}$, can be decomposed into three components, -->

<!-- \begin{align} -->
<!-- \text{relative change in variance}   &=   -->
<!-- \overbrace{\left(1 - R^2_{Y\sim Z|D, {\bf X}}\right)}^\textrm{VRF}  -->
<!-- \underbrace{\left(\frac{ 1}{ 1 - R^2_{D\sim Z | {\bf X}}}\right)}_\textrm{VIF} -->
<!-- \overbrace{\left(\frac{\text{df}}{\text{df}-1}\right)}^\textrm{change in df} \notag\\ -->
<!-- &= \text{VRF} \times \text{VIF} \times \text{change in df}.\label{eq:relvar} -->
<!-- \end{align} -->



## Sensitivity statistics for routine reporting

The previous formulas fully determine the bias (or adjusted values) of the estimate and the standard error for any given degree of confounding, and as such can be used in numerous ways to explore the sensitivity of a regression result. For instance, sensitivity contour plots and sensitivity plots of extreme scenarios, which we demonstrate in the next section using \pkg{sensemakr}, allow us to visualize the entire range of results that would be obtained as we vary both sensitivity parameters. 

Nevertheless, making sensitivity analysis standard practice benefits from simple and interpretable sensitivity measures which can quickly describe the sensitivity of a study to unobserved confounding. These statistics serve two main purposes:

1. They can be easily displayed alongside other usual summary statistics in regression tables, making a minimal sensitivity analysis to unobserved confounding simple, accessible and standardized;

2. They can be easily computed from quantities found in a rregression table, thereby enabling readers and reviwers to assess the sensitivity of results they see in print, even if the original authors did not perform sensitivity analyses.

With this in mind, @cinelli:jrssb2019 propose two main  sensitivity statistics for *routine reporting*: (i) the (observed) partial $R^2$ of the treatment with the outcome, $R^2_{Y\sim D \mid {\bf X}}$; and,  the *robustness value*.

<!-- that can more easily and readily convey the sensitivity of a results in the face of unobserved confounding and can be added to regression tables without having to convey an entire contour plot. -->

### The partial R2 of the treatment with the outcome

Beyond being an effect measure that quantifies how much variation of the outcome the treatment explains, the partial $R^2$ of the treatment with the outcome can also be used to convey how robust the point estimate is to unobserved confounding in an ``extreme scenario''. More precisely, suppose the unobserved confounder $Z$ explains \emph{all} residual variance of the outcome, that is, $R_{Y\sim Z|D, {\bf X}}~=~1$.  Then, for this confounder to bring the point estimate to zero, it must explain  *at least* as much residual variation of the treatment as the residual variation of the outcome that the treatment currently  explains.  In other words, if  $R_{Y\sim Z|D, {\bf X}}~=~1$, then we must have that  $R^2_{D\sim Z|{\bf X}} \geq R^2_{Y\sim D|{\bf X}}$, otherwise this confounder cannot logically account for all the observed association between the treatment and the outcome.


### The Robustness Value

The second sensitivity statistics is the *robustness value*. The robustness value $RV_{q,\alpha}$, measures the *minimal* strength of association that the confounder needs to have, *both* with the treatment and with the outcome, so that a confidence interval of level $\alpha$ includes a change of $q\%$ of the current estimated value.

Let $f_q := q|f_{Y\sim D | {\bf X}}|$, where $|f_{Y\sim D | {\bf X}}|$ is the partial *Cohen's f* of the treatment with the outcome multiplied by the percentage reduction $q$ deemed to be problematic.\footnote{Cohen's $f^2$ can be written as $f^2_{Y\sim D | {\bf X}} = R^2_{Y\sim D | {\bf X}}/(1-R^2_{Y\sim D | {\bf X}})$} Also, let $|t^*_{\alpha, \text{df}-1}|$ denote the t-value threshold for a t-test with significance level of $\alpha$ and $\text{df}-1$ degrees of freedom, and define $f^*_{\alpha, \text{df} - 1} := |t^*_{\alpha, \text{df}-1}|/\sqrt{\text{df} -1}$.   Finally,  construct $f_{q, \alpha}$, which "deducts" from $f_{Y\sim D | {\bf X}}$ both the proportion of reduction $q$ of the point estimate and the boundary below which statistical significance is lost at the level of $\alpha$. That is, 

\begin{align}
f_{q, \alpha} := f_q - f^*_{\alpha,\text{df} - 1}
\end{align}

We then have that $RV_{q,\alpha}$ is given by [@cinelli:jrssb2019; @cinelli:wp2020],

\begin{align}
RV_{q, \alpha} =\left\{ 
\begin{array}{ll}
0, & \text{if}~~f_{q, \alpha} < 0 \\
\frac{1}{2}\left(\sqrt{f_{q, \alpha}^4 + 4f_{q, \alpha}^2} - f_{q, \alpha}^2\right), & \text{if}~~ f_{q} < 1/f^*_{\alpha, \text{df}-1}\\
(f_{q}^2 - f^{*2}_{\alpha,\text{df} - 1})/(1+f^2_{q}), & \text{otherwise}. 
\end{array}\right.
\label{eq:rvt_main}
\end{align}

\noindent Any confounder that explains $RV_{q,\alpha}\%$ of the residual variance *both* of the treatment and of the outcome is sufficiently strong to make the adjusted t-test not reject the null hypothesis $H_0: \tau = (1-q)|\hat{\tau}_{\text{res}}|$ at the $\alpha$ level (or, equivalently, to make the adjusted $1-\alpha$ confidence interval include $(1-q)|\hat{\tau}_{\text{res}}|$). Likewise, a confounder with both associations lower than $RV_{q, \alpha}$ is not capable of overturning the conclusion of such a test. 


## Bounds on the strength of confounding

@cinelli:jrssb2019 employs a bounding approach that uses observed covariates and their strength of association with the treatment and outcome as benchmarks against which to judge hypothetical degrees of confounding, or to argue for bounds on the permissible level of confounding.  Assume first that $Z \perp {\bf X}$ or, equivalently, consider only the part of $Z$ not linearly explained by ${\bf X}$. Suppose a researcher is willing to assume that  some covariate $X_j$ is particularly important to the treatment or outcome, and in fact that omitted variables cannot explain as much residual variance of $D$ or $Y$ as an observed covariate, $X_j$.  More flexibly, one can assert that confounding uniquely explains nor more than $k_D$ times as much of the treatment as does $X_j$ of the treatment, and no more than $k_Y$ as much of the outcome. Formally, this means
\begin{align}
k_D := \frac{R^2_{D\sim Z|{\bf X}_{-j}}}{R^2_{D\sim X_{j}|{\bf X}_{-j}} },  \qquad k_Y := \frac{R^2_{Y \sim Z |{\bf X}_{-j},D}}{R^2_{Y \sim X_{j} |{\bf X}_{-j},D}}.
\end{align}
Where ${\bf X}_{-j}$ represents the vector of covariates ${\bf X}$ excluding $X_{j}$. That is, $k_D$ indexes how much variance of the treatment the confounder explains relative to how much $X_j$ explains (after controlling for the remaining covariates). Ffor example, if the researcher believes the omission of $X_j$  would result in a larger mean squared error of the treatment assignment regression than the omission of $Z$, this equals the claim $k_D \leq 1$. The same reasoning applies to $k_Y$. 

Given parameters $k_D$ and $k_Y$, we can rewrite the strength of the confounders as,
\begin{align}
R^2_{D\sim Z|{\bf X}} = k_D f^2_{D\sim X_{j}|{\bf X}{-j}}, \qquad R^2_{Y\sim Z|D, {\bf X}} \leq \eta^2 f^2_{Y \sim X_j|{\bf X}_{-j},D} \label{eq:bounds}
\end{align}

\noindent where $\eta$ is a scalar which depends on $k_Y$, $k_D$ and $R^2_{D\sim X_{j}|{\bf X}{-j}}$, 

These equations allow us to investigate the maximum bias possible due to confounding at most ``k times'' as strong as a particular covariate $X_j$. 

## Multiple or non-linear confounders



# An illustration of the basic functionality: violence in Darfur

Given that sensitivity analysis requires contextual knowledge to be properly interpreted, we illustrate the basic functionality of the package with a real example. Here we reproduce the results found in Section 5 of [Cinelli and Hazlett (2020)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348), which estimates the effects of exposure to violence on attitudes towards peace, in Darfur. Further details about this application and the data can be found in [Hazlett (2019)]() and [Cinelli and Hazlett (2020)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348).

### The data and research question

During 2003 and 2004, the Darfurian government promoted a horrific violence campaign against civilians, killing an estimated two hundred thousand people. In this application, we are interested in learning how being directly harmed changed individual attitudes towards peace. On average, did direct exposure to this violence make individuals more likely to ask for revenge, or, due to wearieness, ask for peace?

The `sensemakr` package comes with an example dataset drawn from a survey on attitudes of Darfurian refugees in eastern Chad, regarding this unfortunate episode (Hazlett, 2019). The data can be loaded with the command `data("darfur")`.

```{r pkg_data, message=FALSE}
library(sensemakr)
data("darfur")
```

The "treatment" variable of interest is `directlyharmed`, which indicates whether the individual was physically injured during attacks on villages in Darfur, between 2003 and 2004. The main outcome of interest is `peacefactor`, an index measure of pro-peace attitudes.  Other covariates in the data include: `village` (a factor variable indicating the original village of the respondent), `female` (a binary indicator of gender), `age`, `herder_dar` (whether they were a herder in Darfur), `farmer_dar` (whether they were a farmer in Darfur), and `past_voted` (whether they report having voted in an earlier election, prior to the conflict). For further details, see `?darfur`.


Violence against civilians included aerial bombardments by the government as well as assaults by the *Janjaweed*, a pro-government militia. While some villages were selected for more exposure to violence, within village violence was largely indiscriminate. The bombing could not be finely targetted, and the *Janjaweed* had little information about civilians which to attack, with one major exception: women were targeted for sexual assault.

Given these considerations, a researcher may argue that adjusting for `village` and `female` is sufficient for control of confounding, and run the following linear regression model (in which other pre-treatment covariates, although not necessary for identification, are also included):

```{r darfur_model}
darfur.model <- lm(peacefactor ~ directlyharmed  + village +  female +
                                 age + farmer_dar + herder_dar + 
                                 pastvoted + hhsize_darfur, 
                   data = darfur)
```

This regression model results in the following estimates:

```{r stargazer_darfur, echo=FALSE,  results = 'asis'}
stargazer::stargazer(darfur.model, 
                     keep = "directlyharmed", 
                     type = "latex", header = FALSE)
```

That is, we find that, those exposed to violence were on average, more pro-peace. 

### The threat of unobserved confounders

The previous estimate hinges on the assumption of *no unobserved confounders*. Not all investigators, however, may agree with this story. 

For example, one may argue that, athough the bombing was crude, bombs were still more likely to hit the center of the village, and those in the center have already different attitudes towards peace. One may also argue that the *Janjaweed* could have some idea of the wealth of individuals and target those, or that individuals with certain previous political attitudes might have exposed themselves to danger more often. To complicate things, all these factors could be operating non-linearly, such as for instance, with interactions.

This suggest that, instead of the previous linear model ( `darfur.model` ), we should have run the model below,

```{r darfur_complete, eval=FALSE}
darfur.complete.model <- lm(peacefactor ~ directlyharmed  + village +  
                              female + age + farmer_dar + herder_dar + 
                              pastvoted + hhsize_darfur +
                              center*wealth*political_attitudes, 
                            data = darfur)
```

Where here `center*wealth*political_attitudes` is  the `R` formula for including *fully interacted* terms for these three variables. But trying to fit the model `darfur.complete.model` will result in error: none of the variables `center`, `wealth` or `political_attitudes` were measured. Thus, this begs the question: how strong would these unobserved confounders (or *every* reamaining unobserved confounders) need to be to change our previous conclusions? Or, more precisely, how do the inferences regarding the coefficient of `directlyharmed` differ between the model we can actually fit `darfur.model` and the model we *wished* we could have fit `darfur.complete.model`?

Additionally,  we have domain knowledge regarding the main determinants of exposure to violence, such as the special role of gender in targetting. This knowledge may be used to limit the strength of unobserved confounding. For instance, even if variables such as `wealth` remained as confounders, one could argue that it is unreasonable to expect that they explain more of the variation of exposure to violence than gender. How can we leverage claims regarding the relative importance of the variable `female` to bound the plausible strength of unobserved variables? We show next how to answer those questions using `sensemakr`. 


## Sensitivity Analysis

The main function of the package is `sensemakr()`. This function performs the most common sensitivity analyses, which can be then be explored with the print, summary and plot methods (see details in `?print.sensemakr` and `?plot.sensemakr`). We begin the analysis by applying sensemakr to the original regression model, `darfur.model`:

```{r sesemakr_darfur}
darfur.sensitivity <- sensemakr(model = darfur.model, 
                                treatment = "directlyharmed",
                                benchmark_covariates = "female",
                                kd = 1:3,
                                ky = 1:3, 
                                q = 1,
                                alpha = 0.05, 
                                reduce = TRUE)
```

The arguments here are:

- **model**: the `lm` object with the outcome regression. In our case, `darfur.model`.

- **treatment**:  the name of the treatment variable. In our case, `"directlyharmed"`.

- **benchmark_covariates**: the names of covariates that will be used to bound the plausible strength of the unobserved confounders. Here, we put `"female"`, given that we know it was one the main determinants of exposure to violence, and it is also a strong determinant of attitudes towards peace.

- **kd** and **ky**: these arguments parameterize how many times stronger the confounder is related to the treatment ( `kd` ) and to the outcome ( `ky` ) in comparison to the observed benchmark covariate ( `female` ). In our example, this means we want to investigate the maximum strength of a confounder once, twice, or three times as strong as female (in explaining treatment and outcome variation). Default for `ky` is to be the same as `kd`.

- **q**: what percent change the original effect estimate would be deemed problematic? Here 1 means a reduction of 100% of the current effect estimate, that is, a true effect of zero would be deemed problematic. Default is 1.

- **alpha**: significance level of interest for making statistical inferences. Default is 0.05.

- **reduce**: should we consider confounders acting towards *increasing* or *reducing* the absolute value of the estimate? The default is `reduce = TRUE`, which means we are considering confounders  pull the estimate towards (or through) zero.

Using the default arguments, one can simplify the previous call to:

```{r sensemakr_darfur_defaults}
darfur.sensitivity <- sensemakr(model = darfur.model, 
                                treatment = "directlyharmed",
                                benchmark_covariates = "female",
                                kd = 1:3)
```

We can now explore the sensitivity analysis results.

### Minimal sensitivity reporting

The print method of `sensemakr` provides a quick review of the original (unadjusted) estimate along with three summary sensitivity statistics suited for *routine reporting*: the partial R2 of the treatment with the outcome, the robustnuess value (RV) required to reduce the estimate entirely to zero (i.e. $q=1$), and the RV beyond which the estimate would no longer be statistically distinguishable from zero at the 0.05 level ($q=1$, $\alpha=0.05$). 

```{r darfur_print}
darfur.sensitivity
```

The package also provides a function that creates a latex or html table with these results, as shown below (for the html table, simply change the argument to `format = "html"`). 


```{r latex_table_call, eval = FALSE}
ovb_minimal_reporting(darfur.sensitivity, format = "latex")
```

```{r latex_table_real, echo = FALSE, results='asis'}
ovb_minimal_reporting(darfur.sensitivity, format = "latex")
```

These three sensitivity statistics provide a  *minimal reporting* for sensitivity analysis. More precisely:

- The robustness value for bringing the point estimate of `directlyharmed` exactly to zero ($RV_{q=1}$) is 13.9% . This means that unobserved confounders that explain 13.9% of the residual variance  *both* of the treatment and of the outcome are sufficient to explain away all the observed effect. On the other hand,  unobserved confounders that *do not* explain 13.9% of the residual variance *both* of the treatment and of the outcome are not sufficiently strong to do so.

- The robustness value for testing the null hypothesis that the coefficient of `directlyharmed` is zero $(RV_{q =1, \alpha = 0.05})$ falls to 7.6%.  This means that unobserved confounders that explain 7.6% of the residual variance  *both* of the treatment and of the outcome are sufficient to bring  the lower bound of the confidence interval to zero (at the chosen significance level of 5%). On the other hand,  unobserved confounders that *do not* explain 7.6% of the residual variance *both* of the treatment and of the outcome are not sufficiently strong to do so.

- Finally, the partial $R^2$ of `directlyharmed` with `peacefactor` means that, in an *extreme scenario*, in which we assume that unobserved confounders explain *all* of the left out variance of the outcome, these unobserved confounders would need to explain at least 2.2% of the residual variance of the treatment to fully explain away the observed effect.

These are useful quantities that summarize *what you need to know* in order to safely rule out confounders that are deemed to be problematic. Interpreting these values requires domain knowledge about the data generating process. Therefore, we encourage researchers to argue about what are plausible bounds on the maximum explanatory power that unobserved confounders could have in a given application.

Sometimes researchers may have a hard time making judgments regarding the *absolute strength* of a confounder, but may have grounds to make *relative claims*, for instance, by arguing that unobserved confounders are likely not multiple times stronger than a certain observed covariate. In  our application, this is indeed the case. One could argue that, given the nature of the attacks, it is hard to imagine that  unobserved confounding could explain much more of targetting than what was explained by the observed variable `female`. The lower corner of the table, thus, provides bounds on confounding as strong as female, $R^2_{Y\sim Z| {\bf X}, D}$ = 12.5\%, and $R^2_{D\sim Z| {\bf X} }$ = 0.9\%. Since both of those are below the RV, the table reveals that confounders as strong as `female` are not sufficient to explain away the observed estimate. Moreover, since the bound on $R^2_{D\sim Z| {\bf X} }$ is below the partial $R^2$ of the treatment with the outcome, $R^2_{Y \sim D |{\bf X}}$, the table also reveals an extreme confounder explaining all residual variation of the outcome, and as strongly associated with the treatment as `female` also cannot do so.

All these results are exact for a single unobserved confounder, and conservative for multiple confounders, possibly acting non-linearly. Finally, the summary method for `sensemakr` provides an extensive report with verbal descriptions of the results, similar to the explanation we have given above. For further details, please refer to [Cinelli and Hazlett (2020)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348). 

```{r summary, results='hide'}
summary(darfur.sensitivity)
```


### Sensitivity contour plots of point estimates and t-values

The previous sensitivity table  provides a good summary of how robust the current estimate is to unobserved confounding. However, researchers may be willing to refine their analysis by visually exploring the whole range of possible estimates that confounders with different strengths could cause, while placing different bounds on the plausible strength of confounding based on different assumptions on how they compare to observed covariates.  For these, one can use the plot method for `sensemakr`.

As contour plots for the point estimate are the default, we start by examining those.

```{r estimate_plot_call, eval = FALSE}
plot(darfur.sensitivity)
```

The horizontal axis shows the hypothetical residual share of variation of the treatment that unobserved confouding explains, $R^2_{D\sim Z| {\bf X} }$. The vertical axis shows the hypothetical partial $R^2$ of unobserved confouding with the outcome, $R^2_{Y\sim Z| {\bf X}, D}$. The contours show what would be the estimate for `directlyharmed` that one would have obtained in the full regression model including unobserved confounders with such hypothetical strengths. Note the plot is parameterized in way that hurts our preferred hypothesis, by pulling the estimate towards zero---the direction of the bias was set in the argument `reduce = TRUE` of `sensemakr()`.

The bounds on the strength of confouding, determined by the parameter `kd = 1:3` in the call for `sensemakr()`, are also shown in the plot. Note that the plot reveals that the direction of the effect (positive) is robust to confounding once, twice or even three times as strong as the observed covariate `female`, although in this last case the magnitude of the effect is reduced to a third of the original estimate.


We now examine the sensitivity of the *t-value* for testing the null hypothesis of zero effect. For this, it suffices to change the option `sensitivity.of = "t-value"`.


```{r t_plot_call, eval = FALSE}
plot(darfur.sensitivity, sensitivity.of = "t-value")
```

```{r, echo=FALSE}
fig.cap <- "\\label{fig:darfur_contours}Sensitivity contour plots of point estimate (left) and t-value (right)"
```

```{r both_plots_real, echo = FALSE, fig.width=8, fig.height=4, out.width='420px',  out.height='210px', fig.cap=fig.cap}
# make plots appear together
old.par <- par(mfrow = c(1,2))
plot(darfur.sensitivity)
plot(darfur.sensitivity, sensitivity.of = "t-value")
par(old.par)
```

The plot revelas that, at the 5% significance level, the null hypothesis of zero effect would still be rejected given confounders once or twice as strong as `female`. However, differently from the exact point-estimate, accounting for sampling uncertainty now means that the null hypothesis of zero effect *would not* be rejected with the inclusion of a confounder three times as strong as `female`. 

### Sensitivity plots of extreme scenarios

Sometimes researches may be better equipped to make plausibility judgments about the strength of determinants of the treatment assignment mechanism, and have less knowledge about the determinantes of the outcome. In those cases, *extreme scenarios* sensitivity plots may be an option.  For this plot, user should choose the option `type = extreme`. Here one assumes confounding explains **all** or a large fraction of the residual variance of the outcome, and examine how the point estimate is affected under different hypothetical strenghts of the association of the confounder with the treatment. 

```{r darfur_extreme, fig.width=6, out.width="350px"}
plot(darfur.sensitivity, type = "extreme")
```

The default option for the extreme scenarios is `r2yz.dx = c(1, .75, .5)`, which sets the association of confounders  with outcome to  $R^2_{Y\sim Z| {\bf X}, D} = 100\%$,  $R^2_{Y\sim Z| {\bf X}, D}=75\%$  and  $R^2_{Y\sim Z| {\bf X}, D}=50\%$. The bounds on the strength of association of a confounder once, twice or three times as strongly associated with the treatment as `female` are shown as red ticks in the horizonal axis.  As the plot shows, even in the most extreme case of $R^2_{Y\sim Z| {\bf X}, D}=100\%$, confounders would need to be more than twice as strongly associated with the treatment to fully explain away the point estimate. Moving to  the scenarios $R^2_{Y\sim Z| {\bf X}, D}=75\%$  and  $R^2_{Y\sim Z| {\bf X}, D}=50\%$, confounders would need to be more than three times as strongly associated with the treatment to fully explain away the point estimate.

## A disciplined discussion about confounding

Having gone through the basic functionality of the package, here we  recall some caveats on intepretation that applies to any sensitivity analyses. 

The results computed by `sensemakr()` tell us what we need to be prepared to believe, in order to maintain the claim originally made: that exposure to violence, on average, had a large positive effect in attitudes towards peace, in Darfur 2003-2004. In particular, the results of the sensitivity analysis performed here show that, to explain all the observed estimated effect, even in a worst case scenario where the unobserved confounder explains all residual variation of the outcome, this unobserved confounder would need to be at least more than twice as strongly associated with the treatment as the covariate `female`. This is a *true quantitative statement* that describes the strength of confounding needed to overturn the research conclusions. 

The analysis, however, says nothing about whether such a confounder does or does not exist. What it does is to *discipline the discussion* regarding the causal interpretation of the effect estimate:

1. A causal interpretation of the estimate may be defended by articulating that a confounder with such strength is unlikely. For instance, one could argue that, given the way injuries (the "treatment") occurred, the scope for targeting particular types of individuals was quite limited; aircraft dropped makeshift and unguided bombs and other objects over villages, and militia raided without concern for who they are targeting---the only known major exception to this was targeting gender, which is one of the most visually apparent characteristics of an individual, and was one of the main factors of targetting due to sexual assaults. 

2. Likewise, similar grounds are required to persuasively dismiss a causal interpretation of the estimate. There are  standards of (relative) strength that hypothesized unobserved confounders need to meet in order to be problematic. For instance, a skeptic has to articulate (using domain knowledge) why a confounder that explains at least more than twice of the variation of the treatment assignment than the covariate Female is plausible. Otherwise, that confounder cannot logically account for all the observed association, even in an extreme scenario.

That is, robustness to confouding is claimed to the extent one agrees with the arguments in 1 (which rely on domain knowledge about attacks in Darfur); and it can be deemed fragile insofar as alternative stories meeting the requirements in 2 can be made. Therefore, sensitivity analysis are not here to obviate discussions about confouding by following automatic procedures, but to estimulate a more displined, quantitative argument about confounding.





# Customized sensitivity analyses 

The basic functionality demonstrated int he previous section will likely suffice for most users, most of the time. Sometimes, however, more flexibility will be needed in a given project. When this happens, researchers may resort directly to other sensitivity functions of the package to customize their sensitivity analysis. Those functions can be found in the `reference` documentation, and we also provide some examples of those types of analyses next.


Users will most commonly call upon the main functions and related methods, i.e. \code{sensemakr()} and the `plot()`, `print()`, and `summary()` methods for it. The package also gives users access to lower-level functions in order to compute key sensitivity statistics directly. The  \pkg{sensemakr} package is organized:

Users that want to perform more specific sensitivity analysis can use individual functions. These functions can divided in the following categories, according to their functionalities:

- *functions for computing the bias, adjusted estimates and standard errors:* these comprise the functions `bias()`, `relative_bias()`, `bias_factor()`, `adjusted_estimate()`, `adjusted_se()`, `adjusted_t()` and `adjusted_partial_r2()`. These functions take as input the original (unadjested) quantity and a pair of sensitivity parameters (the partial $R^2$ of the omitted variable with the treatment and the outcome), and return the new adjusted quantity reflecting the omitted variable bias. 

- *functions for computing sensitivity statistics*: these comprise the functions `robustness_value()`, `partial_r2()`, `partial_f()`, `partial_f2()` and `sensitivity_stats()`. These functions compute the sensitivity statistics proposed in @cinelli:jrssb2019, such as the robustness value and the partial $R^2$ of the treatment with the outcome, which reveal how robust the regression estimate is to unobserved confounding.

- *bounding functions*: the function `ovb_bound()` computes bounds on the strength of confounding

- *plotting functions*: 

## Revisiting Darfur

Show here how to reproduce with lower level functions:


### Sensitivity statistics

There are individual functions for each of the sensitivity statistics, \code{robustness_value()}, \code{partial_r2()}, and the \code{partial_f()}.  The robustness value and partial $R^2$ have been described above as key sensitivity statistics useful in standard reporting and with direct interpretations.  The partial $f$ does not have as direct an interpretation but appears frequently in the mathematics described above as a sufficient statistic from which sensitivity analyses can be constructed. 

These functions have both \code{lm} and \code{numeric} numeric methods so that users may pass in an \code{lm} object or call them using numerical values based from elsewhere.

```{r}
# lm method
robustness_value(model = darfur.model, covariates = "directlyharmed")
partial_r2(model = darfur.model, covariates = "directlyharmed")
partial_f(model = darfur.model, covariates = "directlyharmed")

# numeric method
robustness_value(t_statistic = 4.18445, dof = 783)
partial_r2(t_statistic = 4.18445, dof = 783)
partial_f(t_statistic = 4.18445, dof = 783)
```

We also provide the convenience function \code{sensitivity_stats()} to compute all three sensitivity statistics for a regression coefficient of interest. The function returns a \code{data.frame}.

```{r}
sensitivity_stats(model = darfur.model, treatment = "directlyharmed", alpha = 0.05)
```

### Bias and adjusted quantities

Several functions are useful to determine biases or adjusted estimates given those biases for postulated degrees of confounding: \code{bias()}, \code{relative_bias()}, \code{ajdusted_estimate()}, \code{adjusted_se()} \code{adjusted_t()}, \code{adjusted_partal_r2()}. Suppose we postulate confounding able to explain 5\% of residual variation in treatment and in the outcome.  We can compute various biases and adjustments directly,

```{r}
bias(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)
relative_bias(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)

adjusted_estimate(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)
adjusted_estimate(estimate = 0.09731582, se = 0.02325654,
                  dof = 783, r2dz.x = 0.05, r2yz.dx = 0.05)

### Adjustment to SE
adjusted_se(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)

### Adjusted t-value 
adjusted_t(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)
```

### Bounds

### Plotting functions

Current default options for plots works best with plots width and height around 4 to 5 inches.

\paragraph{Contour plots}. These can be called directly and bounds/benchmarks can be added to them through additional calls. For example,

```{r}
# contour plot
ovb_contour_plot(model = darfur.model, 
                 treatment = "directlyharmed",
                 benchmark_covariates = "female",
                 kd = 2, nlevels = 5, lim = 0.3)

add_bound_to_contour(r2dz.x = 0.1, 
                     r2yz.dx = 0.1, 
                     bound_value = adjusted_estimate(model = darfur.model,
                                                     treatment = "directlyharmed", 
                                                     r2dz.x = .1, r2yz.dx = .1),
                     bound_label = "Manual Bound")


# add bound 3/1 times stronger than female
add_bound_to_contour(model = darfur.model,
                     treatment = "directlyharmed",
                     benchmark_covariates = "female",
                     kd = 3, ky = 1)

# add bound 50/2 times stronger than age
add_bound_to_contour(model = darfur.model,
                     treatment = "directlyharmed",
                     benchmark_covariates = "age",
                     kd = 50, ky = 2)
```

\paragraph{Extreme scenario plots}

```{r}
ovb_extreme_plot(model = darfur.model, 
                 treatment = "directlyharmed",
                 benchmark_covariates = "female",
                 kd = 1:4,
                 lim = 0.05)
```

## The risks of Informal Benchmarking

In this section, we show how to replicate the example in Section 6.1 of [Cinelli and Hazlett (2020)](https://doi.org/10.1111/rssb.12348) using the R package `sensemakr`. This example is especially useful to illustrate how users can construct their own sensitivity contour plots with customized bounds, and it also shows the risks of informal benchmarking procedures that are still widespread in the sensitivity analysis literature (e.g., Imbens, 2003; Blackwell, 2013; Hosman et al. 2010, Dorie et al., 2016, Hong et al. 2018). 

### Background

Section 4.4 of [Cinelli and Hazlett (2020)](https://doi.org/10.1111/rssb.12348)  shows how it is possible to bound the maximum strength of confounding given relative judgments on how the strength unobserved variables compares to the strength of observed variables. You can compute those bounds with the `sensemakr` package using the function `ovb_bounds()`

Prior work in sensitivity analysis---dating back at least to Imbens (2003), and followed by others (e.g, Hosman et al. 2010, Dorie et al., 2016, Hong et al. 2018)---has also proposed comparing observables with unobservables, but has done so by informally using statistics of observed variables to "calibrate intuitions" about sensitivity parameters concerning the unobserved variable. This practice, however, can have undesirable consequences as we show next.

### Simulating the data

Let us begin by simulating the data generating process used in our example. Consider a treatment variable $D$, an outcome variable $Y$, one observed confounder $X$, and one *unobserved* confounder $Z$. Again, all disturbance variables $U$ are mutually independent and note that, in reality, the treatment $D$ has *no causal effect* on the outcome $Y$. 

\begin{align}
Z &= U_{z}\\
X &= U_{x}\\
D &= X + Z + U_d\\
Y &= X + Z + U_y
\end{align}

Also note that, in this model: (i) the unobserved confounder $Z$ is independent of $X$; and, (ii) the unobserved confounder $Z$ is *exactly like* $X$ in terms of its strength of association with the treatment and the outcome. The code below creates a sample of size 10,000 of this data generating process.


```{r bench_sim_data}
# sets seed for reproducibility
set.seed(12345)

# simulates data
n <- 1e5
Z <- rnorm(n)
X <- rnorm(n)
Z <- resid(lm(Z ~ X)) # makes sure that Z is ortogonal to X
D <- X + Z + rnorm(n)
Y <- X + Z + rnorm(n)
```

### Fitting the model

In this example, the investigator does not observe the confounder $Z$. Therefore, she is forced to fit the restricted linear model $Y \sim D + X$, resulting in the following estimated values

```{r bench_lm}
model.ydx <- lm(Y ~ D + X) 
```

```{r stargazer_bench, echo=FALSE, results='asis'}
stargazer::stargazer(model.ydx, header = FALSE, keep = "D")
```

Note we obtain a large and statistically significant coefficient estimate of the effect of $X$ on $Y$ ($0.5$). However, we know that the variable $Z$ is not observed, and there is the fear that this estimated effect is in fact due to the bias caused by $Z$. On the other hand, let us suppose the investigator correctly know that: (i) $Z$ and $X$ have the same strength of association with $D$ and $Y$; and, (ii) $Z$ is independent of $X$. Can we leverage this information to undersand how much bias a confounder $Z$ "not unlike" $X$ could cause?


### Informal benchmarks

Computing the bias due to the omission of $Z$ requires two sensitivity parameters: its partial $R^2$ with the treatment $D$ and its partial $R^2$ with the outcome $Y$. How could we go about computing the bias that a confounder $Z$ "not unlike $X$" would cause? 

Intuitively, it seems that we could take as reference the observed partial $R^2$ of $X$ with $D$ and $Y$, and use those as the plausible values for the sensitivity parameters. That's the essence of many informal benchmarking proposals. So let us now compute those observed partial $R^2$ using the `partial_r2()` function of `sensemakr`. For the partial $R^2$ of $X$ with the treatment, we also need to fit a treatment regression $D \sim X$ first.

```{r partial_r2_informal}
# fits treatment regression
model.dx <- lm(D ~ X)

# computes observed partial R2 of X
r2yx.d <- partial_r2(model.ydx, covariates = "X")
r2dx   <- partial_r2(model.dx, covariates = "X")
```

Once both partial $R^2$ are computed, we can determine the implied adjusted estimate due to an unobserved confounder $Z$ using the `adjusted_estimate()` function. 

```{r informal_adj_estimate}
informal_adjusted_estimate <- adjusted_estimate(model     = model.ydx, 
                                                treatment = "D", 
                                                r2dz.x    = r2dx, 
                                                r2yz.dx   = r2yx.d)
```

We can now plot the sensitivity contours with `ovb_contour_plot()` and add our informal benchmark with the numeric method of `add_bound_to_contour()`. The arguments `label.bump.x` and `label.bump.y` of these functions allow adjusting the position of the bound label in the plot.

```{r, fig.align='center'}
# draws sensitivity contours
ovb_contour_plot(model = model.ydx,  
                 treatment = "D", 
                 lim = .6)

# adds informal benchmark 
add_bound_to_contour(r2dz.x = r2dx, 
                     r2yz.dx = r2yx.d, 
                     bound_value = informal_adjusted_estimate,
                     bound_label = "Informal benchmark")
```

As we can see, the results of the informal benchmark are different from what we expected. The informal benchmark point is still far away from zero, and this would lead an investigator to incorrectly conclude that an unobserved confounder $Z$ "not unlike $X$" is not sufficient to explain away the observed effect. Moreover, this incorrect conclusion occurs despite *correctly* assuming that: (i) $Z$ and $X$ have the same strength of association with $D$ and $Y$; and, (ii) $Z$ is independent of $X$. Why does this happen?

As explained in Section 6.1 of [Cinelli and Hazlett (2020)](https://doi.org/10.1111/rssb.12348), there are two problems affecting informal benchmarks in this setting. First, we have to make an adjustment of baseline variance to be explained, since the sensitivity parameters consider the partial $R^2$ of $Z$ with the outcome, after taking into account what is already explained by $X$. Second, consider the DAG of our structural model:

\begin{figure}
\centering
\includegraphics[width = 2in, height = 2in]{collider.png}
\caption{DAG of ...}
\end{figure}

That is, although $Z$ is *marginally* independent of $X$, note that $Z$ is not *conditionally* independent of $X$, *given* $D$, because $D$ is a *collider* (Pearl, 2009). This distorts the observed quantities of $X$ that are being used for benchmarking.

### Formal bounds

Given the above considerations, we do not recomment using informal benchmarks for sensitivity analysis. We now show how to compute formal bounds. In `sensemakr`, you can use the function `ovb_bounds()`.

```{r, fig.align='center'}
# compute formal bounds
formal_bound <- ovb_bounds(model = model.ydx, 
                           treatment = "D", 
                           benchmark_covariates = "X", 
                           kd = 1, ky = 1)
```

In this function you specify the linear model being used (`model.ydx`), the treatment of interst ($D$), the observed variable used for benchmarking ($X$), and how stronger $Z$ is in explaining treatment (`kd`) and outcome (`ky`) variation, as compared to the benchmark variable $X$. We can now plot the proper bound against the informal benchmark.

```{r, fig.align='center'}
# contour plot
ovb_contour_plot(model.ydx,  
                 treatment = "D",
                 lim = .6)

add_bound_to_contour(r2dz.x = r2dx, 
                     r2yz.dx = r2yx.d, 
                     bound_value = informal_adjusted_estimate,
                     bound_label = "Informal benchmark")

add_bound_to_contour(bounds = formal_bound,
                     bound_label = "Proper bound")
```

Note that, using the formal bounds, the researcher now reaches the correct conclusion that, an unobserved confounder $Z$ similar to $X$ is strong enough to explain away all the observed association. For further details, please see Sections 4.4 and 6.1 of [Cinelli and Hazlett (2020)](https://doi.org/10.1111/rssb.12348).


# Stata Package

Briefly reproduce Darfur in stata.

# Final Remarks

# References
