---
documentclass: jss
author:
  - name: Carlos Cinelli 
    affiliation: University of California, Los Angeles
    address: >
      Department of Statistics,
      8125 Math Sciences Building,
      Los Angeles, CA 90095, USA.
    email: \email{carloscinelli@ucla.edu}
    url: http://carloscinelli.com
  - name: Jeremy Ferwerda
    affiliation: Dartmouth College
    address: >
      Department of Government, 
      Hanover, NH 03755
    email: \email{jeremy.a.ferwerda@dartmouth.edu}
    url: http://jeremyferwerda.com/

  - name: Chad Hazlett
    affiliation: University of California, Los Angeles
    address: >
      Department of Statistics, 
      8125 Math Sciences Building, 
      Los Angeles, CA 90095, USA.
    email: \email{chazlett@ucla.edu}
    url: http://chadhazlett.com
title:
  # If you use tex in the formatted title, also supply version without
  # For running headers, if needed
  formatted: "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
  plain:     "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
  short:     "\\pkg{sensemakr}: Sensitivity Analysis Tools for OLS"
abstract: "This paper introduces the \\proglang{R} and \\proglang{Stata} package \\pkg{sensemakr} for assesing the sensitivity of regression estimates to unobserved confounding. The package provides a suite of tools for sensitivity analysis in regression models developed in @cinelli:jrssb2019. These tools are based on the familiar omitted variable bias framework, and can be easily computed using only standard regression results. Furthermore, they do not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handle multiple confounders, possibly acting non-linearly, and enable bounding of sensitivity parameters employing domain knowledge."
keywords:
  # at least one keyword must be supplied
  formatted: [causal inference, sensitivity analysis, omitted variable bias, robustness value]
  plain:     [causal inference, sensitivity analysis, omitted variable bias, robustness value]
preamble: >
  \usepackage{amsmath}
output: 
  rticles::jss_article:
    fig_caption: yes
bibliography: sensemakr.bib
biblio-style: jss      #Added Citation style is listed to use in JSS Instructions for Authors.
editor_options: 
  chunk_output_type: console
graphics: yes
---

```{r setup, include=FALSE}
library(knitr)
library(stargazer)

rm(list = ls())

# Set default plot sizes
## - fig.width and fig.heights are R's plot sizes,
## The default options of contours 
## works best with heigh and width around 4 to 5
## - out.width and out.height just resize the image
## 250px seems optimal, and takes less than half a page.
## for extreme plots, change manually fig.width to 6 and out.width to 350px
opts_chunk$set(fig.width  = 4.5, 
               fig.height = 4.5, 
               fig.pos    = "!tp",
               message    = FALSE, 
               out.width  = '250px', 
               out.height = '250px',
               fig.align  = "center")

opts_chunk$set(tidy = FALSE)
options(scipen = 999, digits = 3)
hook_output = knit_hooks$get('output')

knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  n <- options$linewidth
  if (!is.null(n)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```



# Introduction

<!-- Investigating the causal effect of some variable (the "treatment") on another (the "outcome") using observational data poses a perenial problem for researchers across disciplines.  -->
The most common strategy for making causal inferences with observational data is to adjust for observed covariates, and the most common method for doing such adjustment is  (linear) regression. Claiming that the resulting regression coefficient is an unbiased estimate of the causal effect of interest, however, relies on the untestable assumption of no unobserved confounders. Rarely can this assumption be expected to hold exactly, and \emph{qualitative} debates of whether or not \emph{any} unobserved confounding exists are generally not productive---it always does. Therefore, a more useful discussion is \emph{quantitative}: how strong would unobserved confounding need to be to change the research conclusions? Are confounders with such strength plausible? Sensitivity analysis allows us to quantitatively discuss the fragility of putative causal estimates when the underlying assumption of no unobserved confounding is challenged.

The goal of this paper is to introduce the R package \pkg{sensemakr}, whcih implements a suite of tools proposed in @cinelli:jrssb2019 for assessing the sensitivity of a regression coefficient to the inclusion of omitted variables. The goal of \pkg{sensemakr} is to make it easy to understand the impact that omitted variables would have on a regression result. This allows analysts to investigate the robustness of their estimates to violations of the assumption of no unobserved confounding, answering questions such as:

- How strong would an unobserved confounder (or a group of confounders) have to be to change our research conclusions?

- In a worst-case scenario, how robust are our results to all unobserved confounders acting together, possibly non-linearly?

- How strong would confounging need to be relative to the strength of observed covariates, to change our answer a certain amount?



<!-- Weaker claims, e.g. that the causal effect has a particular sign, similarly require the untestable assumption that unobserved confounding does not exceed a certain level. Sensitivity analyses aid in evaluating such claims, by equiping investigators with information about how sensitive a given result is to varying degrees of unobserved confounding, thereby allowing a quantitative discussion regarding the fragility of putative causal estimates when the underlying assumption of no unobserved confounding is challenged. -->

<!-- improving upon our ability to understand and transparently communicate results under confounding. -->


<!-- In the common setting where researchers "adjust for observables" using tools such as regression,  claiming that the resulting estimate is unbiased for a causal quantity rely on untestable assumptions about the absence of unobserved confounders. Weaker claims, e.g. that the causal quantity has a particular sign, similarly require that unobserved confounding does not exceed a certain level. Sensitivity analyses aid in evaluating such claims, by equiping investigators with information about how sensitive a given result is to varying degrees of unobserved confounding, improving upon our ability to understand and transparently communicate results under confounding.  -->


Although several sensitivity analyses have been proposed, dating back to @cornfield1959smoking, with more recent contributions including [@rosenbaum1983assessing; @robins1999association; @frank:smr2000; @rosenbaum2002gamma; @imbens2003sensitivity; @brumback2004sensitivity; @frank:eepa2008; @hosman2010sensitivity; @imai2010identification; @arah2011; @blackwell2013selection; @frank2013would; @carnegie:jree2016; @dorie2016flexible; @middleton2016bias; @oster:jbes2017; @cinelli:icml2019; @franks:jasa2019] Yet, such sensitivity analyses remain underutilized. We argue that a number of factors contribute to this reluctant uptake. One is the complicated nature and strong assumptions many of these methods impose, sometimes involving restrictions on or even a complete description of the nature of the confounder. A second reason is that though users routinely report "regression tables" (or perhaps coefficient plots) to convey the results of a regression, until recently we have lacked "standard" quantities that can simply and correctly summarize sensitivity in the face of unobserved confounding. Third, and most fundamentally, connecting the results of a formal sensitivity analysis to a cogent argument about what types of confounders may exist in one's research project is often difficult, particularly when there are no compelling arguments as to why the treatment assignment should be approximately "ignorable", "exogeneous", or "as-if random".  Further, some of the solutions offered by the literature can lead users to erroneous conclusions.  



<!-- Carlos: I think the quick start ends up not being very useful, because we need to explain everything anyway... -->
<!-- ## Illustration -->

<!-- As a "quick-start" guide, we first show the basic functionality of the package in a simulated example. This functionality will suffice for most users most of the time. The basic workflow is as follows:  -->

<!-- 1) Fit a linear outcome model using `lm.out <- lm()`. This should have your treatment and (pre-treatment) covariates on the right hand side.    -->

<!-- 2) Create a sensemakr object, `sense.out <- sensemakr(lm.out)`, which contains useful sensitivity quantitites. -->

<!-- 3) Explore the results by use of `plot(sense.out)` and `summary(sense.out)`, or through direct calls to the lower-level functions these methods use. -->

<!-- To demonstrate, we first create a simulated dataset. Consider the following linear structural model with a treatment variable $D$, an outcome variable $Y$ and two "confounders" $X$ and $Z$. All disturbance variables $U$ are mutually independent. Note that, in reality, the treatment $D$ has no causal effect on the outcome $Y$.  -->

<!-- \begin{align} -->
<!-- Z &= U_{z}\\ -->
<!-- X &= U_{x}\\ -->
<!-- D &= X + Z + U_d\\ -->
<!-- Y &= X + Z + U_y -->
<!-- \end{align} -->

<!-- The code below creates a sample of size $n = 500$ from this data generating process. -->

<!-- ```{r} -->
<!-- set.seed(10) -->
<!-- n <- 500 -->
<!-- Z <- rnorm(n) -->
<!-- X <- rnorm(n) -->
<!-- D <- X + Z + rnorm(n, sd = 3) -->
<!-- Y <- X + Z + rnorm(n, sd = 3) -->
<!-- ``` -->

<!-- Suppose that an investigator is interested in estimating the causal effect of $D$ on $Y$, but unfortunately the confounder $Z$ is not observed. Despite this, the investigator proceeds with estimation, running a linear regression model adjusting for $X$ only. This results in the following estimates in Table \ref{tab:example}. -->

<!-- ```{r} -->
<!-- lm.model <- lm(Y ~ D + X) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, include = TRUE, results = 'asis'} -->
<!-- stargazer::stargazer(lm.model, header = FALSE,  -->
<!--                      keep = "D",  -->
<!--                      label = "tab:example",  -->
<!--                      title = "restricted OLS results") -->
<!-- ``` -->

<!-- The estimated coefficient for $D$ in the regression model adjusting for $X$ is statistically signficant and large relative to the scale of $D$. The investigator, however, knows that she has not measured all relevant confounders, and that the observed association between $Y$ and $D$ could be due to the omission of some variable $Z$. Further, to aid interpretation or even argue for bounds on confounding, she considers a working assumption that the omitted variable $Z$ is at least as strong as the observed variable $X$, meaning it explains at least as much residual variation in $D$ and $Y$ as $Z$ does.  -->

<!-- How would our estimate vary as we recognize or postulate different degrees of potential confounding? At what point would confounding become strong enough to substantially alter the conclusions of the study? And how can we compare the strength of confounding needed to alter our result to observed covariates to better understand what such confounding implies, or to bound the degree of confounding we think is possible? The \pkg{sensemakr} package provides tools to answer these types of questions. -->

<!-- <!-- Old version: How strong would $Z$ need to be to substantially alter the conclusions of the study? Is that strength plausible? Or, more precisely, how strong would $Z$ need to be relative to the observed covariate $X$? The \pkg{sensemakr} package provides tools to answer these types of questions.!--> -->

<!-- To begin, the investigator passes the estimated model to the function `sensemakr()`, along with the treatment of interest $D$ and optionally an indication of what observed covariates will be used for comparison or "benchmarking", -->
<!-- ```{r, message = FALSE} -->
<!-- library(sensemakr) -->
<!-- sense.model <- sensemakr(model = lm.model,   -->
<!--                          treatment = "D",  -->
<!--                          benchmark_covariates = "X",  -->
<!--                          kd = 1) -->
<!-- sense.model -->
<!-- ``` -->

<!-- The output contains values such as the partial variance in the outcome explained by the treatment, and the "robustness value". For example, the robustness value of 0.14 indicates that confounding explaining less than 14\% of both the treatment and outcome residual variance would not be sufficient to fully account for the estimate. We defer further explanation until below.  Further, full text output in plain English, describing the meaning of these parameters, is provided through `summary(sense.model)`. -->

<!-- The user may also be interested in a number of graphical illustrations of how the estimate varies under hypothesized confounding, including how it would be affected if confounding is "as strong as" the covariate $X$. For example, we can see contour plots of adjusted estimates for the t-statistic, -->


<!-- ```{r, include = FALSE} -->
<!-- cap.example <- "\\label{fig:example} Sensitivity contours of the t-value" -->

<!-- ## plot config -->
<!-- cex.lab = .7 -->
<!-- cex.label.text = .6 -->
<!-- cex.axis = .7 -->
<!-- labcex = .6 -->
<!-- ``` -->

<!-- ```{r, fig.cap= cap.example, fig.pos = "!h", message = FALSE} -->
<!-- plot(sense.model, sensitivity.of = "t-value") -->
<!-- ``` -->


<!-- where the point labeled `1xX` indicates the adjusted estimate as it would appear had confouding been as strong as $X$.  -->


# Sensitivity analysis in an omitted variable bias framework

In this section, we briefly review the omitted variable bias (OVB) framework for sensitivity analysis presented in @cinelli:jrssb2019. This method builds upon a scale-free reparameterization of the OVB formula in terms of partial $R^2$ values. This reparameterization  enables a number of useful analyses, such as: 

- assessing the sensitivity of multiple confounders acting together, possibly non-linearly;
- assessing the sensitivity to extreme scenarios in which all (or a big portion) of the residual variation of the oucome is assumed to be explained by unobserved confounding;
- exploiting knowledge of relative strength of variables to bound the bias due to unobserved confounding;
- presenting all sensitivity results concisely, for easy routine reporting.

<!-- Readers familiar with the method may skip to this section, and read directly the implementation in which we show its implementation with \pkg{sensemakr}. -->

<!--  -->

<!-- This approach shows how the familiar “omitted variable bias” (OVB) framework can be extended to address these challenges in the linear regression setting.  -->
<!-- Notably, all these results do not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounder, and can be used to assess the sensitivity to multiple confounders, whether they influence the treatment and outcome linearly or not.  -->


<!-- Readers that want to see these tools applied in practice, may skip this section. -->


<!-- \textcolor{blue}{CJH: I wouldn't mention these yet -- would define bias and give two formulas then describe RV and partial R2yd.} The “robustness value” describes the minimum strength of association unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial $R^2$ of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates, t-values, as well as ``extreme scenarios'' !-->

## The OVB framework

The starting point of our analysis is a *full* linear regression model of an outcome $Y$ on a treatment $D$, controlling for a set of covariates given by \emph{both} ${\bf X}$ and $Z$,
\begin{align}
Y &= \hat{\tau} D + {\bf X} \hat{{\bf \beta}} +  \hat{\gamma}Z + \hat{\epsilon}_{\text{full}}  \label{eq:fulleq}
\end{align}
\noindent where $Y$ is an $(n \times 1)$ vector containing the outcome of interest for each of the $n$ observations and $D$ is an $(n \times 1)$ treatment variable (which may be continuous or binary); ${\bf X}$ is an $(n \times p)$ matrix of \emph{observed} covariates including the constant; and $Z$ is a single $(n \times 1)$ \emph{unobserved} covariate (we discuss how to extend results for a multivariate $Z$ below).  

Equation \ref{eq:fulleq} is the regresion model that the investigator *wished* she had run to obtain a valid causal estimate of the effect of $D$ on $Y$. Nevertheless,  $Z$ is unobserved. Therefore, the feasible regression the investigator is able to estimate is the *restricted* model \emph{omitting} $Z$, that is,
\begin{align}
Y  &= \hat{\tau}_{\text{res}} D + {\bf X} \hat{{\bf \beta}}_{\text{res}} + \hat{\epsilon}_{\text{res}}\label{eq:restrictedeq}
\end{align}

Given the discrepancy of what we wish to know and what we actually have, the main question we would like to answer is: how do the observed point estimate and standard error of the restricted regression, $\hat{\tau}_{\text{res}}$ and $\widehat{se}(\hat{\tau}_{\text{res}})$, compare to the desired point estimate and standard error of the full regression, $\hat{\tau}$ and $\widehat{se}(\hat{\tau})$?

### OVB with the partial R2 parameterization

<!-- \textcolor{blue}{We can take this route -- FWL to show the classical OVB in the coefficient scale, then switch to R2. Or we can just assert the bias is given by one formula for the estimate and one for the SE. In the Colombia paper, we now do the latter, so its very quick. Here though I like what we have now and lean towards keeping it -- its quick enough that the improvement in understanding is worth it, I think.} !-->

Define as $\widehat{\text{bias}}$ the difference between the full and restricted  estimates, 

\begin{align}
\widehat{\text{bias}}~:=~\hat{\tau}_{\text{\text{res}}}~-~\hat{\tau}
\end{align}

Now let: (i) $R^2_{D\sim Z | {\bf X}}$ denote the share of residual variance of the *treatment* explained by the omitted variable $Z$, after accounting for ${\bf X}$; and, (ii)  $R^2_{Y\sim Z|D, {\bf X}}$ denote the share of residual variance of the *outcome* explained by the omitted variable $Z$, after accounting for ${\bf X}$ and $D$. @cinelli:jrssb2019 have shown that these quantities are sufficient for determining the bias, adjusted estimate and adjusted standard errors of the full regression of Equation \ref{eq:fulleq}.  

More precisely, the bias can be written as,

\begin{align}
|\widehat{\text{bias}}| &= \widehat{\text{se}}(\hat{\tau}_{\text{res}}) \sqrt{\frac{ R^2_{Y\sim Z|D, {\bf X}}~ R^2_{D\sim Z | {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} (\text{df})} \label{eq:r2bias2}
\end{align}

Where $\text{df}$ stands for the degrees of freedom of the restricted regression actually run. Moreover, the estimated standard error of $\hat{\tau}$ can be recoverd with,

\begin{align}
\widehat{\text{se}}(\hat{\tau})  = \widehat{\text{se}}(\hat{\tau}_{\text{res}}) \sqrt{\frac{1 - R^2_{Y\sim Z|D, {\bf X}}}{1 - R^2_{D\sim Z | {\bf X}}} \left(\frac{\text{df}}{\text{df}-1}\right)}.  \label{eq:r2se} 
\end{align}

Given hypothetical values of $R^2_{D\sim Z | {\bf X}}$ and  $R^2_{Y\sim Z|D, {\bf X}}$, Equations \ref{eq:r2bias2} and \ref{eq:r2se} allow investigators to examine the sensitivity of point estimates and standard-errors (and consequently also t-values, confidence intervals or p-values) to the inclusion of any omitted variable $Z$ with such strengths. Or, coversely, given a critical threshold deemed to be problematic, one can find the strength of confounders capable of bringing about a bias of that ammount. Another useful property of the OVB formula with the partial $R^2$ parameterization is that the effect of $R^2_{Y\sim Z|D, {\bf X}}$ on the bias is bounded. This allows investigators to contemplate extreme sensitivity scenarios, in which the parameter $R^2_{Y\sim Z|D, {\bf X}}$ is set to 1 (or another conservativie value), and see what happens as $R^2_{D\sim Z | {\bf X}}$ varies.

## Sensitivity statistics for routine reporting

The previous formulas fully determine the bias (or adjusted values) of the estimate and the standard error for any given degree of confounding, and as such can be used in numerous ways to explore the sensitivity of a regression result. For instance, sensitivity contour plots and sensitivity plots of extreme scenarios, which we demonstrate in the next section using \pkg{sensemakr}, allow us to fully explore the sensitivity of an estimate as we vary both sensitivity parameters. 

Nevertheless, making sensitivity analysis standard practice benefits from simple and interpretable statistics which can quickly describe the sensitivity of a study to unobserved confounding. These statistics serve two main purposes:

1. They can be easily displayed alongside other usual summary statistics in regression tables, making a minimal sensitivity analysis to unobserved confounding simple, accessible and standardized;

2. They can be easily computed from quantities found in a rregression table, thereby enabling readers and reviwers to assess the sensitivity of results they see in print, even if the original authors did not perform sensitivity analyses.

With this in mind, @cinelli:jrssb2019 propose two main  sensitivity statistics for *routine reporting*: (i) the (observed) partial $R^2$ of the treatment with the outcome, $R^2_{Y\sim D \mid {\bf X}}$; and,  the *robustness value*.

<!-- that can more easily and readily convey the sensitivity of a results in the face of unobserved confounding and can be added to regression tables without having to convey an entire contour plot. -->

### The partial R2 of the treatment with the outcome

Beyond being an effect measure that quantifies how much variation of the outcome the treatment explains, the partial $R^2$ of the treatment with the outcome can also be used to convey how robust the point estimate is to unobserved confounding in an ``extreme scenario.'' More precisely, suppose the unobserved confounder $Z$ explains \emph{all} residual variance of the outcome, that is, $R_{Y\sim Z|D, {\bf X}}~=~1$.  Then, for this confounder to bring the point estimate to zero, it must explain  *at least* as much residual variation of the treatment as the residual variation of the outcome that the treatment currently  explains.  In other words, if  $R_{Y\sim Z|D, {\bf X}}~=~1$, then we must have that  $R^2_{D\sim Z|{\bf X}} \geq R^2_{Y\sim D|{\bf X}}$, otherwise this confounder cannot logically account for all the observed association between the treatment and the outcome.


### The Robustness Value

The second sensitivity statistics proposed in @cinelli:jrssb2019 is the *robustness value*. The robustness value $RV_{q,\alpha}$ quantifies the *minimal* strength of association that the confounder needs to have, *both* with the treatment and with the outcome, so that a confidence interval of level $\alpha$ includes a change of $q\%$ of the current estimated value.

Let $f_q := q|f_{Y\sim D | {\bf X}}|$, where $|f_{Y\sim D | {\bf X}}|$ is the partial *Cohen's f* of the treatment with the outcome multiplied by the percentage reduction $q$ deemed to be problematic.\footnote{Cohen's $f^2$ can be written as $f^2_{Y\sim D | {\bf X}} = R^2_{Y\sim D | {\bf X}}/(1-R^2_{Y\sim D | {\bf X}})$} Also, let $|t^*_{\alpha, \text{df}-1}|$ denote the t-value threshold for a t-test with significance level of $\alpha$ and $\text{df}-1$ degrees of freedom, and define $f^*_{\alpha, \text{df} - 1} := |t^*_{\alpha, \text{df}-1}|/\sqrt{\text{df} -1}$.   Finally,  construct $f_{q, \alpha}$, which "deducts" from $f_{Y\sim D | {\bf X}}$ both the proportion of reduction $q$ of the point estimate and the boundary below which statistical significance is lost at the level of $\alpha$. That is, $f_{q, \alpha} := f_q - f^*_{\alpha,\text{df} - 1}$. We then have that $RV_{q,\alpha}$ is given by [@cinelli:jrssb2019; @cinelli:wp2020],

\begin{align}
RV_{q, \alpha} =\left\{ 
\begin{array}{ll}
0, & \text{if}~~f_{q, \alpha} < 0 \\
\frac{1}{2}\left(\sqrt{f_{q, \alpha}^4 + 4f_{q, \alpha}^2} - f_{q, \alpha}^2\right), & \text{if}~~ f_{q} < 1/f^*_{\alpha, \text{df}-1}\\
(f_{q}^2 - f^{*2}_{\alpha,\text{df} - 1})/(1+f^2_{q}), & \text{otherwise}. 
\end{array}\right.
\label{eq:rvt_main}
\end{align}

\noindent Any confounder that explains $RV_{q,\alpha}\%$ of the residual variance *both* of the treatment and of the outcome is sufficiently strong to make the adjusted t-test not reject the null hypothesis $H_0: \tau = (1-q)|\hat{\tau}_{\text{res}}|$ at the $\alpha$ level (or, equivalently, sufficiently strong to make the adjusted $1-\alpha$ confidence interval include $(1-q)|\hat{\tau}_{\text{res}}|$). Likewise, a confounder with both associations lower than $RV_{q, \alpha}$ is not capable of overturning the conclusion of such a test. Setting $\alpha =1$ returns the robustness value for the point estimate.


## Bounds on the strength of confounding

Consider a confounder orthogonal to the observed covariates, ie., $Z \perp {\bf X}$,  or, equivalently, consider only the part of $Z$ not linearly explained by ${\bf X}$. Now denote by $X_j$ a specific covariate of the set ${\bf X}$ and let, 

\begin{align}
k_D := \frac{R^2_{D\sim Z|{\bf X}_{-j}}}{R^2_{D\sim X_{j}|{\bf X}_{-j}} },  \qquad k_Y := \frac{R^2_{Y \sim Z |{\bf X}_{-j},D}}{R^2_{Y \sim X_{j} |{\bf X}_{-j},D}}.
\end{align}

Where ${\bf X}_{-j}$ represents the vector of covariates ${\bf X}$ excluding $X_{j}$. Note that $k_D$ measures the *relative strength* of the observed covariate $X_j$, when compared to the unobserved confounder $Z$,   in terms of the explanatory power of treatment variation (after adjusting for the remaining covariates ${\bf X}_{-j}$). To make things concrete, if, for example, the omission of $X_j$ results in a larger mean squared error in the treatment assignment regression than the omission of $Z$, this means $k_D \leq 1$. Similar interpretation can be given to $k_Y$, *mutatis mutandis*.

Given parameters $k_D$ and $k_Y$, @cinelli:jrssb2019 show that it is possible to rewrite the strength of the confounders as,
\begin{align}
R^2_{D\sim Z|{\bf X}} = k_D f^2_{D\sim X_{j}|{\bf X}{-j}}, \qquad R^2_{Y\sim Z|D, {\bf X}} \leq \eta^2 f^2_{Y \sim X_j|{\bf X}_{-j},D} \label{eq:bounds}
\end{align}

\noindent where $\eta$ is a scalar which depends on $k_Y$, $k_D$ and $R^2_{D\sim X_{j}|{\bf X}{-j}}$.

These equations allow the investigator to assess the maximum bias that a confounder at most ``k times'' as strong as a particular covariate $X_j$ could cause, and thus help understanding, in relative terms, the strength of confounding necessary to be problematic. Furthermore, if the researcher has domain knowledge to argue that a certain covariate $X_j$ is particularly important in explaining treatment or outcome variation, and that omitted variables cannot explain as much residual variance of $D$ or $Y$ as that observed covariate, these results can be used to set plausible bounds in the total amount of confonding.


## Multiple or non-linear confounders

Finally, suppose that, instead of a single unobserved confounder $Z$, there are *multiple* unobserved confounders ${\bf Z} = [Z_1, Z_2, \dots, Z_k]$. In this case, the regression the investigator wished she had run becomes,

\begin{align}
Y &= \hat{\tau} D + {\bf X} \hat{{\bf \beta}} +  {\bf Z}\hat{{\bf \gamma}} + \hat{\epsilon}_{\text{full}}.  \label{eq:fulleq2}
\end{align}

How would the previous sensitivity analyses change? As @cinelli:jrssb2019 show, the previous results considering a single unobserved confounder are in fact *conservative* when considering the impact of multiple confounders, barring an adjustment in the degrees of freedom of Equation \ref{eq:r2se}. Moreover, since the vector ${\bf Z}$ is arbitrary, this can also acommodate non-linear confounders or even misspecification of the functional form of the observed covariates ${\bf X}$. In other words, to assess the maximum bias that multiple, non-linear confounders could cause in our current estimates, it suffices to think in terms of the \emph{maximum explantory power} that ${\bf Z}$ could have have in the treatment and outcome regressions, as parameterized by $R^2_{D\sim {\bf Z} | {\bf X}}$ and $R^2_{Y\sim {\bf Z} |D, {\bf X}}$.


# Basic functionality

In this section we illustrate the basic functionality of the package.  Given that sensitivity analysis requires contextual knowledge to be properly interpreted, we illustrate these tools with a real example. Here we use \pkg{sensemakr} to reproduce all results found in Section 5 of @cinelli:jrssb2019, which estimates the effects of exposure to violence on attitudes towards peace, in Darfur. Further details about this application and the data can be found in @hazlett:jcr2019.

<!-- The main function is `sensemakr()`, and it aims to provide a simple interface that will suffice for most users. The function performs the most common sensitivity analyses, which can then be further explored with the print, plot and summary methods, as we illustrate below.  -->

##  Violence in Darfur: data and research question

In 2003 and 2004, the Darfurian government orchestrated a horrific campaign of violence against civilians, killing an estimated two hundred thousand people. This application asks whether, on average, being directly injured or maimed in this episode made individuals more likely to feel “vengeful” and unwilling to make peace with those who perpetrated this violence. Or, were those who directly suffered from such violence most motivated to see it end by making peace?



The `sensemakr` package comes with an example dataset drawn from a survey on attitudes of Darfurian refugees in eastern Chad [@hazlett:jcr2019]. To get started we first need to install the package. From within \proglang{R}, the \pkg{sensemakr} package can be installed from the Comprehensive \proglang{R} Archive Network (CRAN).

```{r install_sensemakr, eval = FALSE}
install.packages("sensemakr")
```

Next, after loading the package, the data can be loaded with the command `data("darfur")`.

```{r pkg_data, message=FALSE}
library(sensemakr)
data("darfur")
```

The "treatment" variable of interest is `directlyharmed`, which indicates whether the individual was physically injured or maimed during the attack on her or his village in Darfur. The main outcome of interest is `peacefactor`, an index measuring pro-peace attitudes.  Other covariates in the data include: `village` (a factor variable indicating the original village of the respondent), `female` (a binary indicator of gender), `age`, `herder_dar` (whether they were a herder in Darfur), `farmer_dar` (whether they were a farmer in Darfur), and `past_voted` (whether they report having voted in an earlier election, prior to the conflict). For further details, see `?darfur`.


The purpose of these attacks was to punish civilians from ethnic groups presumed to support the opposition, and to kill or drive these groups out so as to reduce this support. Violence against civilians included aerial bombardments by the government as well as assaults by the *Janjaweed*, a pro-government militia.  For this example, suppose a researcher argues that, while some villages were more or less intensively attacked, within village violence was largely indiscriminate. The bombings could not be finely targeted owing to their crudeness, and there were not many reason to target them.  Similarly, the *Janjaweed* had no reason to target certain individuals rather than others, and no information with which to do so---with one major exception: women were targeted and often subjected to sexual violence. 


Given these considerations, this researcher may argue that adjusting for `village` and `female` is sufficient for control of confounding, and run the following linear regression model (in which other pre-treatment covariates, although not necessary for identification, are also included):

```{r darfur_model}
darfur.model <- lm(peacefactor ~ directlyharmed  + village +  female +
                                 age + farmer_dar + herder_dar + 
                                 pastvoted + hhsize_darfur, 
                   data = darfur)
```

This regression model results in the estimates shown in Table \ref{tab:darfur_ols}. According to this model, those who were directly harmed in violence were on average more "pro-peace," not less.

\begin{table}
\centering
```{r stargazer_darfur, echo=FALSE,  results = 'asis'}
stargazer::stargazer(darfur.model, 
                     keep = c("directlyharmed", "female"), 
                     type = "latex", 
                     header = FALSE, 
                     float = F)
```
\caption{OLS results for \texttt{darfur.model}. Due to sapce, only the coefficients for \texttt{directlyharmed} and \texttt{female} are shown.}
\label{tab:darfur_ols}
\end{table}



### The threat of unobserved confounders

The previous estimate requires the assumption of *no unobserved confounders* for unbiasedness. While supported by the claim that there is no targeting of violence within village and gender strata, not all investigators may agree with this account. For example, one may argue that, athough the bombing was crude, bombs were still more likely to hit the center of the village, and those in the center were also likely to hold different attitudes towards peace. One may also argue that the *Janjaweed* may observe signals that indicate, for example, the wealth of individuals. Or, perhaps, that an individual's prior political attitudes could have led them to take actions that exposed them to greater risks. To complicate things, all these factors could interact with each other or otherwise have other non-linear effects.

These concerns suggest that, instead of the previous linear model (`darfur.model`), we should have run a model such as

```{r darfur_complete, eval=FALSE}
darfur.complete.model <- lm(peacefactor ~ directlyharmed  + village +  
                              female + age + farmer_dar + herder_dar + 
                              pastvoted + hhsize_darfur +
                              center*wealth*political_attitudes, 
                            data = darfur)
```

Where here `center*wealth*political_attitudes` is  the `R` formula for including *fully interacted* terms for these three variables. 

However trying to fit the model `darfur.complete.model` will result in error: none of the variables `center`, `wealth` or `political_attitudes` were measured. We can nevertheless ask questions such as "how strong would these unobserved confounders (or *all* reamaining unobserved confounderß) need to be to change our previous conclusions?" Or, more precisely, given an assumption on how strongly these and other omitted variables relate to the treatment and the outcome, how would including them have changed our inferences regarding the coefficient of `directlyharmed`? These are precisely the types of questions that can be answered with `sensemakr`.

Additionally,  we have domain knowledge regarding the main determinants of exposure to violence, such as the special role of gender in targetting. This knowledge can be used to either aid in interpretation of the magnitude of confounding required to change the research conclusions, or to impose limits upon the strength of unobserved confounding.  For instance, even if variables such as `wealth` remained as confounders, one could argue that it is unreasonable to expect that they explain more of the variation of exposure to violence than does gender.  We show next how `sensemakr` can leverage such claims to bound the plausible strength of unobserved variables.

<!-- How can we leverage claims regarding the relative importance of the variable `female` to bound the plausible strength of unobserved variables? We show next how to answer those questions using `sensemakr`.  -->


## Violence in Darfur: sensitivity analysis

The main function of the package is `sensemakr()`. This function performs the most commonly required sensitivity analyses, which can then be  further explored with the print, summary and plot methods (see details in `?print.sensemakr` and `?plot.sensemakr`). We begin the analysis by applying `sensemakr` to the original regression model, `darfur.model`:

```{r sesemakr_darfur}
darfur.sensitivity <- sensemakr(model = darfur.model, 
                                treatment = "directlyharmed",
                                benchmark_covariates = "female",
                                kd = 1:3,
                                ky = 1:3, 
                                q = 1,
                                alpha = 0.05, 
                                reduce = TRUE)
```

The arguments of this call are:

- **model**: the `lm` object with the outcome regression. In our case, `darfur.model`.

- **treatment**:  the name of the treatment variable. In our case, `"directlyharmed"`.

- **benchmark_covariates**: the names of covariates that will be used to bound the plausible strength of the unobserved confounders. Here, we put `"female"`, which one could argue to be among the main determinants of exposure to violence, and it also found to be among the strongest determinants of attitudes towards peace.

- **kd** and **ky**: these arguments parameterize how many times stronger the confounder is related to the treatment ( `kd` ) and to the outcome ( `ky` ) in comparison to the observed benchmark covariate ( `"female"` ). In our example, setting `kd = 1:3` and `ky = 1:3` means we want to investigate the maximum strength of a confounder once, twice, or three times as strong as female (in explaining treatment and outcome variation).  If only `kd` is given, `ky` will be set equal to it by default.

- **q**: this allows the user to specify what fraction of the effect estimate would have to be explained away to be problematic.  Setting `q = 1`, as we do here, means that a reduction of 100% of the current effect estimate, that is, a true effect of *zero*, would be deemed problematic. The default is `q = 1`.

- **alpha**: significance level of interest for making statistical inferences. The default is set to `alpha = 0.05`.

- **reduce**:  should we consider confounders acting towards *increasing* or *reducing* the absolute value of the estimate? The default is `reduce = TRUE`, which means we are considering confounders that pull the estimate towards (or through) zero. Setting `reduce = FALSE` will consider confunders that pull the estimate *away* from zero.

Using the default arguments, one can simplify the previous call to

```{r sensemakr_darfur_defaults}
darfur.sensitivity <- sensemakr(model = darfur.model, 
                                treatment = "directlyharmed",
                                benchmark_covariates = "female",
                                kd = 1:3)
```

After running `sensemakr()`, we can explore the sensitivity analysis results.

### Minimal sensitivity reporting

The print method of `sensemakr` provides a quick review of the original (unadjusted) estimate along with three summary sensitivity statistics suited for *routine reporting*: (1) the partial R2 of the treatment with the outcome; (2) the robustnuess value (RV) required to reduce the estimate entirely to zero (i.e. $q=1$); and, (3) the RV beyond which the estimate would no longer be statistically distinguishable from zero at the 0.05 level ($q=1$, $\alpha=0.05$). 

```{r darfur_print}
darfur.sensitivity
```

The package also provides a function that creates a latex or html table with these results, as shown in Table \ref{tab:minimal} (for the html table, simply change the argument to `format = "html"`). 


```{r latex_table_call, eval = FALSE}
ovb_minimal_reporting(darfur.sensitivity, format = "latex")
```

\begin{table}
\centering
\begin{tabular}{lrrrrrr}
\multicolumn{7}{c}{Outcome: \textit{peacefactor}} \\
\hline \hline 
Treatment: & Est. & S.E. & t-value & $R^2_{Y \sim D |{\bf X}}$ & $RV_{q = 1}$ & $RV_{q = 1, \alpha = 0.05}$  \\ 
\hline 
\textit{directlyharmed} & 0.097 & 0.023 & 4.184 & 2.2\% & 13.9\% & 7.6\% \\ 
\hline 
df = 783 & & \multicolumn{5}{r}{ \small \textit{Bound (1x female)}: $R^2_{Y\sim Z| {\bf X}, D}$ = 12.5\%, $R^2_{D\sim Z| {\bf X} }$ = 0.9\%} \\
\end{tabular}
\caption{Minimal sensitivity analysis reporting.}
\label{tab:minimal}
\end{table}

These three sensitivity statistics provide a  *minimal reporting* for sensitivity analysis. More precisely:

- The robustness value for bringing the point estimate of `directlyharmed` exactly to zero ($RV_{q=1}$) is 13.9%. This means that unobserved confounders that explain 13.9% of the residual variance *both* of the treatment and of the outcome are sufficiently strong to explain away all the observed effect. On the other hand, unobserved confounders that *do not* explain at least 13.9% of the residual variance *both* of the treatment and of the outcome are not sufficiently strong to do so.

- The robustness value for testing the null hypothesis that the coefficient of `directlyharmed` is zero $(RV_{q =1, \alpha = 0.05})$ falls to 7.6%.  This means that unobserved confounders that explain 7.6% of the residual variance  *both* of the treatment and of the outcome are sufficiently strong to bring the lower bound of the confidence interval to zero (at the chosen significance level of 5%). On the other hand,  unobserved confounders that *do not* explain at least 7.6% of the residual variance *both* of the treatment and of the outcome are not sufficiently strong to do so.

- Finally, the partial $R^2$ of `directlyharmed` with `peacefactor` means that, in an *extreme scenario*, in which we assume that unobserved confounders explain *all* of the left out variance of the outcome, these unobserved confounders would need to explain at least 2.2% of the residual variance of the treatment to fully explain away the observed effect.

These are useful quantities that summarize *what we need to know* in order to safely rule out confounders that are deemed to be problematic. Interpreting these values requires domain knowledge about the data generating process. Therefore, we encourage researchers to argue about what are plausible bounds on the maximum explanatory power that unobserved confounders could have in a given application.

Sometimes researchers may have a hard time making judgments regarding the *absolute strength* of a confounder, but may have grounds to make *relative claims*, for instance, by arguing that unobserved confounders are likely not multiple times stronger than a certain observed covariate. In  our application, this is indeed the case. One could argue that, given the nature of the attacks, it is hard to imagine that  unobserved confounding could explain much more of targetting than what was explained by the observed variable `female`. 

The lower corner of the table, thus, provides bounds on confounding as strong as female, $R^2_{Y\sim Z| {\bf X}, D}$ = 12.5\%, and $R^2_{D\sim Z| {\bf X} }$ = 0.9\%. Since both of those are below the RV, the table reveals that confounders as strong as `female` are not sufficient to explain away the observed estimate. Moreover, the bound on $R^2_{D\sim Z| {\bf X} }$ is below the partial $R^2$ of the treatment with the outcome, $R^2_{Y \sim D |{\bf X}}$. This means that even an extreme confounder explaining all residual variation of the outcome and as strongly associated with the treatment as `female` would not be able to overturn the research conclusions

As mentioned in the previous section, all these results are exact for a single unobserved confounder, and conservative for multiple confounders, possibly acting non-linearly. Finally, the summary method for `sensemakr` provides an extensive report with verbal descriptions verbal descriptions of all these analyses. Here, for instance, entering `summary(darfur.sensitivity)` produces verbose output similar to the text explanations in the last several paragraphs, so that researchers can directly cite or include such text in their reports.

<!-- For further details, please refer to [Cinelli and Hazlett (2020)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssb.12348).  -->

<!-- ```{r summary, results='hide'} -->
<!-- summary(darfur.sensitivity) -->
<!-- ``` -->


### Sensitivity contour plots of point estimates and t-values

The previous sensitivity table  provides a good summary of how robust the current estimate is to unobserved confounding. However, researchers may be willing to refine their analysis by visually exploring the whole range of possible estimates that confounders with different strengths could cause, while placing different bounds on the plausible strength of confounding based on different assumptions on how they compare to observed covariates.  For these, one can use the plot method for `sensemakr`.

We begin by examining the default plot type, contour plots for the point estimate.

```{r estimate_plot_call, eval = FALSE}
plot(darfur.sensitivity)
```

The resulting plot is shown in the left of Figure \ref{fig:darfur_contours}. The horizontal axis shows the hypothetical residual share of variation of the treatment that unobserved confouding explains, $R^2_{D\sim Z| {\bf X} }$. The vertical axis shows the hypothetical partial $R^2$ of unobserved confouding with the outcome, $R^2_{Y\sim Z| {\bf X}, D}$. The contours show what would be the estimate for `directlyharmed` that one would have obtained in the full regression model including unobserved confounders with such hypothetical strengths. Note the plot is parameterized in way that hurts our preferred hypothesis, by pulling the estimate towards zero---the direction of the bias was set in the argument `reduce = TRUE` of `sensemakr()`.

The bounds on the strength of confouding, determined by the parameter `kd = 1:3` in the call for `sensemakr()`, are also shown in the plot. Note that the plot reveals that the direction of the effect (positive) is robust to confounding once, twice or even three times as strong as the observed covariate `female`, although in this last case the magnitude of the effect is reduced to a third of the original estimate.


We now examine the sensitivity of the *t-value* for testing the null hypothesis of zero effect. For this, it suffices to change the option `sensitivity.of = "t-value"`.


```{r t_plot_call, eval = FALSE}
plot(darfur.sensitivity, sensitivity.of = "t-value")
```

```{r fig_cap_contour, echo=FALSE}
fig.cap <- "\\label{fig:darfur_contours}Sensitivity contour plots of point estimate (left) and t-value (right)"
```

```{r both_plots_real, echo = FALSE, fig.width=8, fig.height=4, out.width='420px',  out.height='210px', fig.cap=fig.cap}
# make plots appear together
old.par <- par(mfrow = c(1,2))
plot(darfur.sensitivity)
plot(darfur.sensitivity, sensitivity.of = "t-value")
par(old.par)
```

The resulting plot is shown in the right of Figure \ref{fig:darfur_contours}.  The plot reveals that, at the 5% significance level, the null hypothesis of zero effect would still be rejected given confounders once or twice as strong as `female`. However, by contrast to the point-estimate, accounting for sampling uncertainty now means that the null hypothesis of zero effect *would not* be rejected with the inclusion of a confounder three times as strong as `female`. 

### Sensitivity plots of extreme scenarios

Sometimes researchers may be better equipped to make plausibility judgments about the strength of determinants of the treatment assignment mechanism, and have less knowledge about the determinantes of the outcome. In those cases,  sensitivity plots using *extreme scenarios* are a useful option. These are produced with the option `type = extreme`. Here one assumes confounding explains **all** or  some large fraction of the residual variance of the outcome, then vary how strongly such confounding is hypothetically related to the treatment, to see how this affects the resulting point estimate.

```{r fig_cap_extreme, echo = FALSE}
fig.cap.extreme <- "\\label{fig:extreme}Sensitivity analysis to extreme scenarios."
```


```{r darfur_extreme, fig.width=6, out.width="350px", fig.cap=fig.cap.extreme ,fig.pos="t"}
plot(darfur.sensitivity, type = "extreme")
```

The plot is shown in Figure \ref{fig:extreme}. The default option for the extreme scenarios is `r2yz.dx = c(1, .75, .5)`, which sets the association of confounders with the outcome to  $R^2_{Y\sim Z| {\bf X}, D} = 100\%$,  $R^2_{Y\sim Z| {\bf X}, D}=75\%$  and  $R^2_{Y\sim Z| {\bf X}, D}=50\%$, producing three separate curves for each scenario. The bounds on the strength of association of a confounder once, twice or three times as strongly associated with the treatment as `female` are shown as red ticks in the horizonal axis.  As the plot shows, even in the most extreme case of $R^2_{Y\sim Z| {\bf X}, D}=100\%$, confounders would need to be more than twice as strongly associated with the treatment to fully explain away the point estimate. Moving to  the scenarios $R^2_{Y\sim Z| {\bf X}, D}=75\%$  and  $R^2_{Y\sim Z| {\bf X}, D}=50\%$, confounders would need to be more than three times as strongly associated with the treatment to fully explain away the point estimate.

Having demonstrated the basic functionality of the package, we end this section by recalling some important caveats on intepretation that apply to any sensitivity analyses. Readers may refer to @cinelli:jrssb2019 for further discussion. 

## A disciplined discussion about confounding


The results computed by `sensemakr()` tell us what we need to be prepared to believe in order to sustain that a given conclusion is not due to confounding. In particular, the results of the sensitivity analysis performed here show that, to explain all the observed estimated effect, even in a worst case scenario where the unobserved confounder explains all residual variation of the outcome, this unobserved confounder would need to be more than twice as strongly associated with the treatment as the covariate `female`. This is a *true quantitative statement* that describes the strength of confounding needed to overturn the research conclusions.  

The analysis, however, says nothing about whether such a confounder does or does not exist. The role of sensitivity analysis is, therefore, to *discipline the discussion* regarding the causal interpretation of the effect estimate. In particular,

1. A causal interpretation of the estimate may be defended by articulating that a confounder with such strength is unlikely. For instance, one could argue that, given the way injuries (the "treatment") occurred, the scope for targeting particular types of individuals was quite limited; aircraft dropped makeshift and unguided bombs and other objects over villages, and militia raided without concern for who they are targeting---the only known major exception to this was targeting gender, which is one of the most visually apparent characteristics of an individual, and was a strong factor in targetting. 

2. Likewise, similar grounds are required to persuasively dismiss a causal interpretation of the estimate. There are standards of (relative) strength that the hypothesized unobserved confounders need to meet in order to be problematic. For instance, a skeptic has to articulate (using domain knowledge) why a confounder that explains at least more than twice of the variation of the treatment assignment than the covariate `female` is plausible. Otherwise, that confounder cannot logically account for all the observed association, even in an extreme scenario.

Robustness in the face of confouding is thus claimed to the extent one agrees with the arguments in 1 (which rely on domain knowledge about attacks in Darfur), while a result can be deemed fragile insofar as alternative stories meeting the requirements in 2 can be offered. Sensitivity analyses should not be used to obviate discussions about confouding by engaging in automatic procedures; rather, they should stimulate a more displined, quantitative argument about confounding, in which such statements are made and properly debated.

### Going further

The basic functionality demonstrated in this section will likely suffice for most users, most of the time. Sometimes, however, more flexibility will be needed in a given project. When this happens, researchers may resort directly to other sensitivity functions of the package to customize their sensitivity analyses, which we discuss next. 

# Customized sensitivity analyses 

Users that want to perform customized sensitivity analyses, beyond what is offered by default in `sensemakr()`, can do so by using the individual functions of the package. These functions can be divided in the following categories, according to their functionalities:

- *functions for computing the bias, adjusted estimates and standard errors:* these comprise, among others, the functions `bias()`,  `adjusted_estimate()`, `adjusted_se()` and `adjusted_t()`. These functions take as input the original (unadjusted) estimate (in the form of a linear model or numeric values) and a pair of sensitivity parameters (the partial $R^2$ of the omitted variable with the treatment and the outcome), and return the new quantity adjusted for omitted variable bias. 

- *functions for computing sensitivity statistics*: these comprise, among others, the functions `partial_r2()`,  `robustness_value()`, and `sensitivity_stats()`. These functions compute sensitivity statistics suited for routine reporting, as proposed in @cinelli:jrssb2019. These functions take as input the original (unadjusted) estimate (in the form of a linear model or numeric values), and return the corresponding sensitivity statistic.

- *sensitivity plots*: These functions provide direct access to sensitivity contour plots `ovb_contour_plot()` and sensitivity plots of extreme scenarios `ovb_extreme_plot()` for customization. There is also a convenience function `add_bound_to_contour()` which faciliates the placement of user computed bounds on contour plots. All plot functions return invisibly the data needed to replicate the plot, so users that prefer fully customized plots can also easily do so. The default options for plots works best with width and height around 4 to 5 inches.

- *bounding functions*: these functions computes bounds on the strength of confounding "k times" as strong as certain observed convariates. The main high level function is `ovb_bounds()`, and there is also the auxiliary function `ovb_partial_r2_bound()`.

In this section, we illustrate these functionalities via examples. 

## Customized bounds

### Background:  informal Benchmarking

Informal “benchmarking” procedures have been widely suggested in the sensitivity analysis literature as a means to aid interpretation. It intends to describe how an unobserved confounder $Z$ “not unlike” some observed covariate $X_j$ would alter the results of a study (e.g., Imbens, 2003; Blackwell, 2013; Hosman et al. 2010, Dorie et al., 2016, Hong et al. 2018). \cite{cinelli:jrssb2019} show why these proposals may lead users to erroneous conclusions, and offer formal bounds on the bias that could be produced by unobserved confounding “as strong” as certain observed covariates (as we have reviewed here in Section XXX). Here we replicate the example in Section 6.1 of  @cinelli:jrssb2019, in which such benchmarks produce clearly misleading results. Replicating this example with `sensemakr` provides a useful tutorial on how users can construct their own sensitivity contour plots with customized bounds, beyond what is offered by default on the package.


###  Data and model

We begin by simulating the data generating process which will be used in our example, as given by Equations \ref{eq:naive_first} to \ref{eq:naive_last} below. Here we have a treatment variable $D$, an outcome variable $Y$, one observed confounder $X$, and one *unobserved* confounder $Z$. All disturbance variables $U$ are standardized mutually independent gaussians. Note that, in reality, the treatment $D$ has *no causal effect* on the outcome $Y$. 

\begin{align}
Z &= U_{z}\label{eq:naive_first}\\
X &= U_{x}\\
D &= X + Z + U_d\\
Y &= X + Z + U_y \label{eq:naive_last}
\end{align}

Also note that, in this model: (i) the unobserved confounder $Z$ is independent of $X$; and, (ii) the unobserved confounder $Z$ is *exactly like* $X$ in terms of its strength of association with the treatment and the outcome. The code below creates a sample of size 100 of this data generating process. We use the function `resid_maker()` to make sure the residuals are standardized and orthogonal, thus all properties that we describe here hold exactly even in this finite sample.

```{r resid_maker,echo=FALSE}
# function for orthogonalizing and
# standardizing residuals
resid_maker <- function(n, C){
  e <- resid(lm(rnorm(n) ~ C))
  e  <- c(scale(e))
  return(e)
}
```

```{r naive_dgp}
n <- 100
X <- scale(rnorm(n))
Z <- resid_maker(n, X) 
D <- X + Z + resid_maker(n, cbind(X, Z)) 
Y <- X + Z + resid_maker(n, cbind(X, Z, D))
```

In this example, the investigator knows she need to adjust for the confounder $Z$ but, unfortunately, does not observe $Z$. Therefore, she is forced to fit the restricted linear model adjusting for $X$ only. 

```{r bench_lm}
model.ydx <- lm(Y ~ D + X) 
```

This regression gives the following estimates, show in the first column of Table \ref{tab:naive}. 

\begin{table}
\centering

```{r stargazer_bench, echo=FALSE, results='asis'}
model.ydxz <- lm(Y ~ D +X + Z)
stargazer::stargazer(model.ydx, model.ydxz,
                     header = FALSE, 
                     keep = c("D","X", "Z"),
                     column.labels = c("Restricted OLS", "Full OLS"),
                     float = FALSE)
```
\caption{First column: results of the restricted regression adusting for $X$ only. Second column: results of the full regression adusting for $X$ and $Z$.}
\label{tab:naive}
\end{table}

That is, the researcher obtains a large and statistically significant coefficient estimate of the effect of $X$ on $Y$. However there is the fear that the observed association is in fact solely due to the bias caused by the omission of $Z$. 

### Formal benchmarks

Let us suppose the investigator *correctly* knows that: (i) $Z$ and $X$ have the same strength of association with $D$ and $Y$; and, (ii) $Z$ is independent of $X$. How can she leverage this information to undersand how much bias a confounder $Z$ "not unlike" $X$ could cause? 

As we have seen in Section XXX, it is possible to use the observed partial $R^2$ of $X$ with $D$ and $Y$ with Equation \ref{eq:bounds} to obtain valid bounds on the amount of confounding caused by an unobserved $Z$ as strongly associated with the treatment and with the outcome as $X$.  

In `sensemakr`, this can be done with the function `ovb_bounds()`. In this function one needs to specify the linear model being used (`model = model.ydx`), the treatment of interest (`treatment = "D"`), the observed variable used for benchmarking (`benchmark_covariates = "X"`), and how stronger $Z$ is in explaining treatment (`kd = 1`) and outcome (`ky = 1`) variation, as compared to the benchmark variable $X$.

```{r formal_bound}
formal_bound <- ovb_bounds(model = model.ydx, 
                           treatment = "D", 
                           benchmark_covariates = "X", 
                           kd = 1, 
                           ky = 1)
```

We can now inspect the output of `ovb_bounds()`.

```{r formal_bound_no_print, eval = FALSE}
formal_bound
```
```{r formal_bound_print, echo = FALSE}
numeric <- sapply(formal_bound, is.numeric)
formal_bound[numeric] <- round(formal_bound[numeric], 3)
rownames(formal_bound) <- NULL
formal_bound[1:6]
```

As we can see, the results of the bounding procedure shows us that, an unobserved confounder $Z$, "not unlike $X$",  would: (1) explain 50% of the residual variation of the treatment and 33% of the residual variation of the outcome; (2) bring the point estimate exactly to zero; and, (3) bring the standard error to 0.1. And this is precisely what we have if one runs the full regression model adjusting for $X$ and $Z$, as shown in the second column of Table \ref{tab:naive}.


### Informal benchmarks

Computing the bias due to the omission of $Z$ requires two sensitivity parameters: its partial $R^2$ with the treatment $D$ and its partial $R^2$ with the outcome $Y$.  Intuitively, it seems that, instead of appealing to  Equation \ref{eq:bounds}, one could take a simpler route: take as reference the observed partial $R^2$ of $X$ with $D$ and $Y$, and use those, without adjustment, as the plausible values for the sensitivity parameters. That is the essence of many informal benchmarking proposals. So let us compute these informal bounds.

First, we can obtain the observed partial $R^2$ of $X$ with $Y$ using the `partial_r2()` function of `sensemakr` in the `model.ydx` regression.

```{r partial_r2_y}
r2yx.d <- partial_r2(model.ydx, covariates = "X")

```

To obtain the partial $R^2$ of $X$ with the treatment, we need to fit a treatment regression $D \sim X$ first.

```{r partial_r2_d}
model.dx <- lm(D ~ X)
r2dx   <- partial_r2(model.dx, covariates = "X")
```

Once both partial $R^2$ are computed, we can determine the implied adjusted estimate due to an unobserved confounder $Z$ using the `adjusted_estimate()` function. 

```{r informal_adj_estimate}
informal_adjusted_estimate <- adjusted_estimate(model     = model.ydx, 
                                                treatment = "D", 
                                                r2dz.x    = r2dx, 
                                                r2yz.dx   = r2yx.d)
```

We can now compare the informal benchmarks with the previous formal bounds. We first plot the sensitivity contours with `ovb_contour_plot()`. Next, we the informal benchmark with the numeric method of the function `add_bound_to_contour()`. Finally, we add the formal bounds previously computed. When needed, users can customize the position of the bound label in the plot with the arguments `label.bump.x` and `label.bump.y`. Several other graphical customizations are available.


```{r fig_cap_naive, echo=FALSE}
fig.cap.naive <- fig.cap <- "\\label{fig:naive_benchmarking}Informal benchmarking \\emph{versus} proper bounds."
```

```{r informal_benchmark_plot, fig.align='center', fig.cap = fig.cap.naive}
# draws sensitivity contours
ovb_contour_plot(model = model.ydx,  
                 treatment = "D", 
                 lim = .6)

# adds informal benchmark 
add_bound_to_contour(r2dz.x = r2dx, 
                     r2yz.dx = r2yx.d, 
                     bound_value = informal_adjusted_estimate,
                     bound_label = "Informal benchmark")

# adds formal bound
add_bound_to_contour(bounds = formal_bound, 
                     bound_label = "Formal bound")
```

As we can see, the results of the informal benchmark are different from what we expected. The informal benchmark point is still far away from zero, and this would lead an investigator to incorrectly conclude that an unobserved confounder $Z$ "not unlike $X$" is not sufficient to explain away the observed effect. Moreover, this incorrect conclusion occurs despite *correctly* assuming that: (i) $Z$ and $X$ have the same strength of association with $D$ and $Y$; and, (ii) $Z$ is independent of $X$. Why does this happen?

As explained in Section 6.1 of @cinelli:jrssb2019, there are two problems affecting informal benchmarks in this setting. First, we have to make an adjustment of baseline variance to be explained, since the sensitivity parameters consider the partial $R^2$ of $Z$ with the outcome, after taking into account what is already explained by $X$. Second, consider the DAG of our structural model, as shown in Figure \ref{fig:dag_naive}

\begin{figure}
\centering
\includegraphics[width = 4in, height = 4in]{collider.png}
\caption{DAG of the model given by Equations \ref{eq:naive_first} to \ref{eq:naive_last}. Note that, although, marginally $Z$ is independent of $X$, conditioning on $D$ creates dependency between these two variables, since $D$ is a collider (Pearl, 2009).}
\label{fig:dag_naive}
\end{figure}

That is, although $Z$ is *marginally* independent of $X$, note that $Z$ is not *conditionally* independent of $X$, *given* $D$, because $D$ is a *collider* (Pearl, 2009). Conditioning on $D$ creates an association between $Z$ and $X$, and this distorts the observed quantities of $X$ that are being used for benchmarking.  Given the above considerations, we do not recomment using informal benchmarks for sensitivity analysis. As we have seen before, note that, using the formal bounds, the researcher now reaches the correct conclusion: an unobserved confounder $Z$ similar to $X$ is strong enough to explain away all the observed association. For further details and discussion, we point readers to Sections 4.4 and 6.1 of @cinelli:jrssb2019.



## Revisiting Darfur

Show here how to reproduce with lower level functions:


### Sensitivity statistics

There are individual functions for each of the sensitivity statistics, \code{robustness_value()}, \code{partial_r2()}, and the \code{partial_f()}.  The robustness value and partial $R^2$ have been described above as key sensitivity statistics useful in standard reporting and with direct interpretations.  The partial $f$ does not have as direct an interpretation but appears frequently in the mathematics described above as a sufficient statistic from which sensitivity analyses can be constructed. 

These functions have both \code{lm} and \code{numeric} numeric methods so that users may pass in an \code{lm} object or call them using numerical values based from elsewhere.

```{r}
# lm method
robustness_value(model = darfur.model, covariates = "directlyharmed")
partial_r2(model = darfur.model, covariates = "directlyharmed")
partial_f(model = darfur.model, covariates = "directlyharmed")

# numeric method
robustness_value(t_statistic = 4.18445, dof = 783)
partial_r2(t_statistic = 4.18445, dof = 783)
partial_f(t_statistic = 4.18445, dof = 783)
```

We also provide the convenience function \code{sensitivity_stats()} to compute all three sensitivity statistics for a regression coefficient of interest. The function returns a \code{data.frame}.

```{r}
sensitivity_stats(model = darfur.model, treatment = "directlyharmed", alpha = 0.05)
```

### Bias and adjusted quantities

Several functions are useful to determine biases or adjusted estimates given those biases for postulated degrees of confounding: \code{bias()}, \code{relative_bias()}, \code{ajdusted_estimate()}, \code{adjusted_se()} \code{adjusted_t()}, \code{adjusted_partal_r2()}. Suppose we postulate confounding able to explain 5\% of residual variation in treatment and in the outcome.  We can compute various biases and adjustments directly,

```{r}
bias(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)
relative_bias(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)

adjusted_estimate(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)
adjusted_estimate(estimate = 0.09731582, se = 0.02325654,
                  dof = 783, r2dz.x = 0.05, r2yz.dx = 0.05)

### Adjustment to SE
adjusted_se(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)

### Adjusted t-value 
adjusted_t(darfur.model, treatment = "directlyharmed", r2dz.x = 0.05, r2yz.dx = 0.05)
```

### Bounds

### Plotting functions


\paragraph{Contour plots}. These can be called directly and bounds/benchmarks can be added to them through additional calls. For example,

```{r}
# contour plot
ovb_contour_plot(model = darfur.model, 
                 treatment = "directlyharmed",
                 benchmark_covariates = "female",
                 kd = 2, nlevels = 5, lim = 0.3)

add_bound_to_contour(r2dz.x = 0.1, 
                     r2yz.dx = 0.1, 
                     bound_value = adjusted_estimate(model = darfur.model,
                                                     treatment = "directlyharmed", 
                                                     r2dz.x = .1, r2yz.dx = .1),
                     bound_label = "Manual Bound")


# add bound 3/1 times stronger than female
add_bound_to_contour(model = darfur.model,
                     treatment = "directlyharmed",
                     benchmark_covariates = "female",
                     kd = 3, ky = 1)

# add bound 50/2 times stronger than age
add_bound_to_contour(model = darfur.model,
                     treatment = "directlyharmed",
                     benchmark_covariates = "age",
                     kd = 50, ky = 2)
```

\paragraph{Extreme scenario plots}

```{r}
ovb_extreme_plot(model = darfur.model, 
                 treatment = "directlyharmed",
                 benchmark_covariates = "female",
                 kd = 1:4,
                 lim = 0.05)
```


# Stata Package

Briefly reproduce Darfur in stata.

# Final Remarks

# References
